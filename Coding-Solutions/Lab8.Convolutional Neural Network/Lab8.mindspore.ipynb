{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-16 17:54:04--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 170052171 (162M) [application/x-gzip]\r\n",
      "Saving to: ‘cifar-10-binary.tar.gz’\r\n",
      "\r\n",
      "\rcifar-10-binary.tar   0%[                    ]       0  --.-KB/s               \rcifar-10-binary.tar   0%[                    ]   8.00K  32.9KB/s               \rcifar-10-binary.tar   0%[                    ]  40.00K  82.2KB/s               \rcifar-10-binary.tar   0%[                    ] 104.00K   142KB/s               \rcifar-10-binary.tar   0%[                    ] 232.00K   238KB/s               \rcifar-10-binary.tar   0%[                    ] 472.00K   388KB/s               \rcifar-10-binary.tar   0%[                    ] 968.00K   663KB/s               \rcifar-10-binary.tar   1%[                    ]   1.90M  1.11MB/s               \rcifar-10-binary.tar   2%[                    ]   3.82M  1.96MB/s               \rcifar-10-binary.tar   4%[                    ]   6.80M  3.10MB/s               \rcifar-10-binary.tar   5%[>                   ]   9.12M  3.60MB/s               \rcifar-10-binary.tar   5%[>                   ]   9.16M  3.29MB/s               \rcifar-10-binary.tar   7%[>                   ]  12.12M  3.99MB/s    eta 38s    \rcifar-10-binary.tar   8%[>                   ]  13.76M  4.25MB/s    eta 38s    \rcifar-10-binary.tar   9%[>                   ]  15.01M  4.22MB/s    eta 38s    \rcifar-10-binary.tar  10%[=>                  ]  17.05M  4.54MB/s    eta 38s    \rcifar-10-binary.tar  11%[=>                  ]  18.01M  4.52MB/s    eta 38s    \rcifar-10-binary.tar  11%[=>                  ]  19.32M  4.24MB/s    eta 34s    \rcifar-10-binary.tar  13%[=>                  ]  22.32M  4.65MB/s    eta 34s    \rcifar-10-binary.tar  15%[==>                 ]  24.38M  4.84MB/s    eta 34s    \rcifar-10-binary.tar  15%[==>                 ]  24.99M  4.95MB/s    eta 34s    \rcifar-10-binary.tar  16%[==>                 ]  26.96M  5.33MB/s    eta 34s    \rcifar-10-binary.tar  17%[==>                 ]  28.10M  5.56MB/s    eta 28s    \rcifar-10-binary.tar  18%[==>                 ]  29.24M  5.32MB/s    eta 28s    \rcifar-10-binary.tar  19%[==>                 ]  31.38M  5.67MB/s    eta 28s    \rcifar-10-binary.tar  20%[===>                ]  32.45M  5.77MB/s    eta 28s    \rcifar-10-binary.tar  20%[===>                ]  33.52M  5.70MB/s    eta 28s    \rcifar-10-binary.tar  21%[===>                ]  34.63M  5.34MB/s    eta 28s    \rcifar-10-binary.tar  22%[===>                ]  35.76M  5.46MB/s    eta 28s    \rcifar-10-binary.tar  22%[===>                ]  36.90M  5.11MB/s    eta 28s    \rcifar-10-binary.tar  23%[===>                ]  38.04M  4.95MB/s    eta 27s    \rcifar-10-binary.tar  24%[===>                ]  39.21M  5.02MB/s    eta 27s    \rcifar-10-binary.tar  24%[===>                ]  40.40M  4.77MB/s    eta 27s    \rcifar-10-binary.tar  25%[====>               ]  41.59M  5.15MB/s    eta 27s    \rcifar-10-binary.tar  26%[====>               ]  42.79M  4.63MB/s    eta 27s    \rcifar-10-binary.tar  27%[====>               ]  43.99M  4.70MB/s    eta 25s    \rcifar-10-binary.tar  27%[====>               ]  45.20M  4.93MB/s    eta 25s    \rcifar-10-binary.tar  28%[====>               ]  46.15M  4.53MB/s    eta 25s    \rcifar-10-binary.tar  29%[====>               ]  47.07M  4.79MB/s    eta 25s    \rcifar-10-binary.tar  29%[====>               ]  48.01M  4.72MB/s    eta 25s    \rcifar-10-binary.tar  30%[=====>              ]  48.99M  4.80MB/s    eta 24s    \rcifar-10-binary.tar  30%[=====>              ]  50.18M  4.81MB/s    eta 24s    \rcifar-10-binary.tar  31%[=====>              ]  51.43M  4.93MB/s    eta 24s    \rcifar-10-binary.tar  32%[=====>              ]  52.68M  4.92MB/s    eta 24s    \rcifar-10-binary.tar  33%[=====>              ]  53.93M  5.06MB/s    eta 24s    \rcifar-10-binary.tar  33%[=====>              ]  54.93M  5.10MB/s    eta 22s    \rcifar-10-binary.tar  34%[=====>              ]  55.88M  5.01MB/s    eta 22s    \rcifar-10-binary.tar  35%[======>             ]  56.85M  5.10MB/s    eta 22s    \rcifar-10-binary.tar  35%[======>             ]  57.82M  5.02MB/s    eta 22s    \rcifar-10-binary.tar  36%[======>             ]  59.02M  5.12MB/s    eta 22s    \rcifar-10-binary.tar  37%[======>             ]  60.27M  5.22MB/s    eta 21s    \rcifar-10-binary.tar  37%[======>             ]  61.27M  5.21MB/s    eta 21s    \rcifar-10-binary.tar  38%[======>             ]  62.20M  5.21MB/s    eta 21s    \rcifar-10-binary.tar  38%[======>             ]  63.18M  5.11MB/s    eta 21s    \rcifar-10-binary.tar  39%[======>             ]  64.16M  5.16MB/s    eta 21s    \rcifar-10-binary.tar  40%[=======>            ]  65.37M  5.14MB/s    eta 20s    \rcifar-10-binary.tar  41%[=======>            ]  66.63M  5.27MB/s    eta 20s    \rcifar-10-binary.tar  41%[=======>            ]  67.63M  5.29MB/s    eta 20s    \rcifar-10-binary.tar  42%[=======>            ]  68.57M  5.17MB/s    eta 20s    \rcifar-10-binary.tar  42%[=======>            ]  69.55M  5.21MB/s    eta 20s    \rcifar-10-binary.tar  43%[=======>            ]  70.54M  5.14MB/s    eta 19s    \rcifar-10-binary.tar  44%[=======>            ]  71.73M  5.24MB/s    eta 19s    \rcifar-10-binary.tar  45%[========>           ]  72.99M  5.28MB/s    eta 19s    \rcifar-10-binary.tar  45%[========>           ]  73.99M  5.29MB/s    eta 19s    \rcifar-10-binary.tar  46%[========>           ]  74.93M  5.23MB/s    eta 19s    \rcifar-10-binary.tar  46%[========>           ]  75.92M  5.18MB/s    eta 18s    \rcifar-10-binary.tar  47%[========>           ]  76.88M  5.21MB/s    eta 18s    \rcifar-10-binary.tar  48%[========>           ]  78.10M  5.18MB/s    eta 18s    \rcifar-10-binary.tar  48%[========>           ]  79.37M  5.25MB/s    eta 18s    \rcifar-10-binary.tar  49%[========>           ]  80.65M  5.18MB/s    eta 18s    \rcifar-10-binary.tar  50%[=========>          ]  81.93M  5.25MB/s    eta 16s    \rcifar-10-binary.tar  51%[=========>          ]  83.21M  5.30MB/s    eta 16s    \rcifar-10-binary.tar  51%[=========>          ]  84.21M  5.31MB/s    eta 16s    \rcifar-10-binary.tar  52%[=========>          ]  85.16M  5.26MB/s    eta 16s    \rcifar-10-binary.tar  53%[=========>          ]  86.10M  5.18MB/s    eta 16s    \rcifar-10-binary.tar  53%[=========>          ]  87.16M  5.24MB/s    eta 15s    \rcifar-10-binary.tar  54%[=========>          ]  88.40M  5.22MB/s    eta 15s    \rcifar-10-binary.tar  55%[==========>         ]  89.70M  5.36MB/s    eta 15s    \rcifar-10-binary.tar  55%[==========>         ]  90.73M  5.33MB/s    eta 15s    \rcifar-10-binary.tar  56%[==========>         ]  91.66M  5.25MB/s    eta 15s    \rcifar-10-binary.tar  57%[==========>         ]  92.65M  5.28MB/s    eta 14s    \rcifar-10-binary.tar  57%[==========>         ]  93.73M  5.24MB/s    eta 14s    \rcifar-10-binary.tar  58%[==========>         ]  95.01M  5.35MB/s    eta 14s    \rcifar-10-binary.tar  59%[==========>         ]  96.30M  5.41MB/s    eta 14s    \rcifar-10-binary.tar  60%[===========>        ]  97.30M  5.41MB/s    eta 14s    \rcifar-10-binary.tar  60%[===========>        ]  98.34M  5.39MB/s    eta 13s    \rcifar-10-binary.tar  61%[===========>        ]  99.40M  5.34MB/s    eta 13s    \rcifar-10-binary.tar  61%[===========>        ] 100.54M  5.42MB/s    eta 13s    \rcifar-10-binary.tar  62%[===========>        ] 101.90M  5.43MB/s    eta 13s    \rcifar-10-binary.tar  63%[===========>        ] 103.27M  5.60MB/s    eta 13s    \rcifar-10-binary.tar  64%[===========>        ] 104.32M  5.56MB/s    eta 11s    \rcifar-10-binary.tar  64%[===========>        ] 105.38M  5.51MB/s    eta 11s    \rcifar-10-binary.tar  65%[============>       ] 106.52M  5.59MB/s    eta 11s    \rcifar-10-binary.tar  66%[============>       ] 107.73M  5.58MB/s    eta 11s    \rcifar-10-binary.tar  67%[============>       ] 109.18M  5.74MB/s    eta 11s    \rcifar-10-binary.tar  68%[============>       ] 110.66M  5.82MB/s    eta 10s    \rcifar-10-binary.tar  68%[============>       ] 111.80M  5.89MB/s    eta 10s    \rcifar-10-binary.tar  69%[============>       ] 112.99M  5.88MB/s    eta 10s    \rcifar-10-binary.tar  70%[=============>      ] 114.20M  5.86MB/s    eta 10s    \rcifar-10-binary.tar  71%[=============>      ] 115.48M  5.98MB/s    eta 10s    \rcifar-10-binary.tar  72%[=============>      ] 117.05M  6.06MB/s    eta 9s     \rcifar-10-binary.tar  73%[=============>      ] 118.68M  6.26MB/s    eta 9s     \rcifar-10-binary.tar  73%[=============>      ] 119.93M  6.25MB/s    eta 9s     \rcifar-10-binary.tar  74%[=============>      ] 121.24M  6.27MB/s    eta 9s     \rcifar-10-binary.tar  75%[==============>     ] 122.55M  6.37MB/s    eta 9s     \rcifar-10-binary.tar  76%[==============>     ] 123.99M  6.42MB/s    eta 7s     \rcifar-10-binary.tar  77%[==============>     ] 125.15M  6.14MB/s    eta 7s     \rcifar-10-binary.tar  78%[==============>     ] 126.59M  6.17MB/s    eta 7s     \rcifar-10-binary.tar  79%[==============>     ] 128.27M  6.38MB/s    eta 7s     \rcifar-10-binary.tar  79%[==============>     ] 129.32M  6.29MB/s    eta 7s     \rcifar-10-binary.tar  80%[===============>    ] 130.65M  6.29MB/s    eta 6s     \rcifar-10-binary.tar  81%[===============>    ] 132.01M  6.26MB/s    eta 6s     \rcifar-10-binary.tar  82%[===============>    ] 133.41M  6.19MB/s    eta 6s     \rcifar-10-binary.tar  83%[===============>    ] 134.84M  6.25MB/s    eta 6s     \rcifar-10-binary.tar  83%[===============>    ] 135.89M  6.15MB/s    eta 6s     \rcifar-10-binary.tar  84%[===============>    ] 136.93M  6.16MB/s    eta 5s     \rcifar-10-binary.tar  85%[================>   ] 138.07M  6.05MB/s    eta 5s     \rcifar-10-binary.tar  85%[================>   ] 139.35M  6.03MB/s    eta 5s     \rcifar-10-binary.tar  86%[================>   ] 140.63M  5.65MB/s    eta 5s     \rcifar-10-binary.tar  87%[================>   ] 142.15M  5.61MB/s    eta 4s     \rcifar-10-binary.tar  88%[================>   ] 143.87M  5.78MB/s    eta 4s     \rcifar-10-binary.tar  89%[================>   ] 144.60M  5.54MB/s    eta 4s     \rcifar-10-binary.tar  89%[================>   ] 145.52M  5.38MB/s    eta 4s     \rcifar-10-binary.tar  90%[=================>  ] 146.62M  5.62MB/s    eta 4s     \rcifar-10-binary.tar  91%[=================>  ] 147.77M  5.41MB/s    eta 3s     \rcifar-10-binary.tar  91%[=================>  ] 148.96M  5.38MB/s    eta 3s     \rcifar-10-binary.tar  92%[=================>  ] 150.16M  5.35MB/s    eta 3s     \rcifar-10-binary.tar  93%[=================>  ] 151.37M  5.24MB/s    eta 3s     \rcifar-10-binary.tar  94%[=================>  ] 152.59M  5.37MB/s    eta 3s     \rcifar-10-binary.tar  94%[=================>  ] 153.49M  5.17MB/s    eta 2s     \rcifar-10-binary.tar  95%[==================> ] 154.37M  5.22MB/s    eta 2s     \rcifar-10-binary.tar  95%[==================> ] 155.27M  5.14MB/s    eta 2s     \rcifar-10-binary.tar  96%[==================> ] 156.37M  5.05MB/s    eta 2s     \rcifar-10-binary.tar  97%[==================> ] 157.63M  5.06MB/s    eta 2s     \rcifar-10-binary.tar  97%[==================> ] 158.91M  5.26MB/s    eta 1s     \rcifar-10-binary.tar  98%[==================> ] 160.21M  5.03MB/s    eta 1s     \rcifar-10-binary.tar  99%[==================> ] 161.51M  5.09MB/s    eta 1s     \rcifar-10-binary.tar 100%[===================>] 162.17M  5.02MB/s    in 31s     \r\n",
      "\r\n",
      "2022-11-16 17:54:36 (5.25 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\r\n",
      "\r\n",
      "cifar-10-batches-bin/\r\n",
      "cifar-10-batches-bin/data_batch_1.bin\r\n",
      "cifar-10-batches-bin/batches.meta.txt\r\n",
      "cifar-10-batches-bin/data_batch_3.bin\r\n",
      "cifar-10-batches-bin/data_batch_4.bin\r\n",
      "cifar-10-batches-bin/test_batch.bin\r\n",
      "cifar-10-batches-bin/readme.html\r\n",
      "cifar-10-batches-bin/data_batch_5.bin\r\n",
      "cifar-10-batches-bin/data_batch_2.bin\r\n"
     ]
    }
   ],
   "source": [
    "# 这里我们做一个更强的训练\n",
    "!wget -c \"https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\"\n",
    "!tar -zxvf \"cifar-10-binary.tar.gz\"\n",
    "!mv \"cifar-10-batches-bin\" \"./datasets/cifar-10-batches-bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset import vision\n",
    "from mindspore.dataset.transforms.transforms import TypeCast\n",
    "import mindspore.dataset.engine as de # 数据增强引擎。\n",
    "\n",
    "def create_cifar_dataset(dataset_path, do_train, batch_size=32, image_size=(224, 224), rank_size=1, rank_id=0):\n",
    "    dataset = ds.Cifar10Dataset(dataset_path, shuffle=do_train,\n",
    "                                num_shards=rank_size, shard_id=rank_id)\n",
    "\n",
    "    # define map operations\n",
    "    trans = []\n",
    "    # 训练的时候增强数据\n",
    "    if do_train:\n",
    "        trans += [\n",
    "            # vision.RandomCrop((32, 32), (4, 4, 4, 4)),\n",
    "            # vision.RandomHorizontalFlip(prob=0.5)\n",
    "            vision.AutoAugment(vision.AutoAugmentPolicy.CIFAR10),# 这是经过实验得出的最优策略\n",
    "        ]\n",
    "\n",
    "    trans += [\n",
    "        vision.Resize(image_size),\n",
    "        vision.Rescale(1.0 / 255.0, 0.0),\n",
    "        vision.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    type_cast_op = TypeCast(ms.int32)\n",
    "\n",
    "    data_set = dataset.map(operations=type_cast_op, input_columns=\"label\")\n",
    "    data_set = data_set.map(operations=trans, input_columns=\"image\")\n",
    "\n",
    "    # apply batch operations\n",
    "    data_set = data_set.batch(batch_size, drop_remainder=do_train)\n",
    "    return data_set\n",
    "path = 'datasets/cifar-10-batches-bin'\n",
    "train_loader, test_loader = [create_cifar_dataset(path, do_train) for do_train in [True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms.set_context(mode=ms.context.GRAPH_MODE, device_target=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.argv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mindspore_hub as mshub\n",
    "# # model = \"mindspore/1.9/res2net50_cifar10\" # 这个模型也有，不过就没有意思了\n",
    "# model = \"mindspore/1.9/resnet50_imagenet2012\" # 我们体验一下迁移学习\n",
    "# # model = \"mindspore/1.6/googlenet_cifar10\"\n",
    "# network = mshub.load(model, include_top=False, activation=\"Sigmoid\", num_classes=10)\n",
    "# network.set_train(False) # 这个API比pytorch直观很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-16 18:13:53--  https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/source-codes/resnet.py\n",
      "Resolving obs.dualstack.cn-north-4.myhuaweicloud.com (obs.dualstack.cn-north-4.myhuaweicloud.com)... 121.36.121.131, 121.36.121.130, 2407:c080:170f:fffb:1:1:2:5, ...\r\n",
      "Connecting to obs.dualstack.cn-north-4.myhuaweicloud.com (obs.dualstack.cn-north-4.myhuaweicloud.com)|121.36.121.131|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 9521 (9.3K) [binary/octet-stream]\r\n",
      "Saving to: ‘resnet.py’\r\n",
      "\r\n",
      "\rresnet.py             0%[                    ]       0  --.-KB/s               \rresnet.py           100%[===================>]   9.30K  --.-KB/s    in 0.01s   \r\n",
      "\r\n",
      "2022-11-16 18:13:53 (960 KB/s) - ‘resnet.py’ saved [9521/9521]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -N https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/source-codes/resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import resnet50\n",
    "network = resnet50(batch_size=32, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits\n",
    "# 这个损失函数的名字比torch清晰一些\n",
    "ls = SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "opt = nn.Momentum(filter(lambda x: x.requires_grad, network.get_parameters()), 0.01, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "import os\n",
    "from mindspore import Model\n",
    "model = Model(network, loss_fn=ls, optimizer=opt, metrics={'acc'})\n",
    "\n",
    "steps_per_epoch = train_loader.get_dataset_size()\n",
    "config_ck = ms.CheckpointConfig(save_checkpoint_steps=steps_per_epoch, \n",
    "                                keep_checkpoint_max=16)\n",
    "ckpt_cb = ms.ModelCheckpoint(prefix='CIFAR-10-resnet50', \n",
    "                             directory='./checkpoint/ms', config=config_ck)\n",
    "locc_monitor = ms.LossMonitor(1) # 每个step打印一次loss\n",
    "time_monitor = TimeMonitor(steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import SummaryCollector\n",
    "summary_collector = SummaryCollector(summary_dir='./summary_dir', collect_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(9443:140165025510336,MainProcess):2022-11-16-18:32:24.753.375 [mindspore/train/callback/_summary_collector.py:739] Can not get graph proto, it may not be 'GRAPH_MODE' in context currently, so SummaryCollector will not collect graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 2.291393280029297\n",
      "epoch: 1 step: 2, loss is 2.2892565727233887\n",
      "epoch: 1 step: 3, loss is 2.274559736251831\n",
      "epoch: 1 step: 4, loss is 2.3019986152648926\n",
      "epoch: 1 step: 5, loss is 2.327470302581787\n",
      "epoch: 1 step: 6, loss is 2.2708733081817627\n",
      "epoch: 1 step: 7, loss is 2.3094005584716797\n",
      "epoch: 1 step: 8, loss is 2.2858121395111084\n",
      "epoch: 1 step: 9, loss is 2.287644624710083\n",
      "epoch: 1 step: 10, loss is 2.2976865768432617\n",
      "epoch: 1 step: 11, loss is 2.2875921726226807\n",
      "epoch: 1 step: 12, loss is 2.2821764945983887\n",
      "epoch: 1 step: 13, loss is 2.2654058933258057\n",
      "epoch: 1 step: 14, loss is 2.325120210647583\n",
      "epoch: 1 step: 15, loss is 2.300588369369507\n",
      "epoch: 1 step: 16, loss is 2.2634737491607666\n",
      "epoch: 1 step: 17, loss is 2.3121283054351807\n",
      "epoch: 1 step: 18, loss is 2.3303170204162598\n",
      "epoch: 1 step: 19, loss is 2.341153144836426\n",
      "epoch: 1 step: 20, loss is 2.308833599090576\n",
      "epoch: 1 step: 21, loss is 2.3046936988830566\n",
      "epoch: 1 step: 22, loss is 2.303579568862915\n",
      "epoch: 1 step: 23, loss is 2.333793878555298\n",
      "epoch: 1 step: 24, loss is 2.3062448501586914\n",
      "epoch: 1 step: 25, loss is 2.319120168685913\n",
      "epoch: 1 step: 26, loss is 2.2963316440582275\n",
      "epoch: 1 step: 27, loss is 2.279423952102661\n",
      "epoch: 1 step: 28, loss is 2.3213798999786377\n",
      "epoch: 1 step: 29, loss is 2.2616958618164062\n",
      "epoch: 1 step: 30, loss is 2.2793967723846436\n",
      "epoch: 1 step: 31, loss is 2.2996366024017334\n",
      "epoch: 1 step: 32, loss is 2.2777090072631836\n",
      "epoch: 1 step: 33, loss is 2.2609498500823975\n",
      "epoch: 1 step: 34, loss is 2.3171091079711914\n",
      "epoch: 1 step: 35, loss is 2.2763774394989014\n",
      "epoch: 1 step: 36, loss is 2.338653326034546\n",
      "epoch: 1 step: 37, loss is 2.3277738094329834\n",
      "epoch: 1 step: 38, loss is 2.294588088989258\n",
      "epoch: 1 step: 39, loss is 2.250223159790039\n",
      "epoch: 1 step: 40, loss is 2.2550575733184814\n",
      "epoch: 1 step: 41, loss is 2.2919960021972656\n",
      "epoch: 1 step: 42, loss is 2.2674107551574707\n",
      "epoch: 1 step: 43, loss is 2.2821152210235596\n",
      "epoch: 1 step: 44, loss is 2.3126962184906006\n",
      "epoch: 1 step: 45, loss is 2.309210777282715\n",
      "epoch: 1 step: 46, loss is 2.2322568893432617\n",
      "epoch: 1 step: 47, loss is 2.2710940837860107\n",
      "epoch: 1 step: 48, loss is 2.2812137603759766\n",
      "epoch: 1 step: 49, loss is 2.204333782196045\n",
      "epoch: 1 step: 50, loss is 2.281140089035034\n",
      "epoch: 1 step: 51, loss is 2.230790853500366\n",
      "epoch: 1 step: 52, loss is 2.25374174118042\n",
      "epoch: 1 step: 53, loss is 2.254281997680664\n",
      "epoch: 1 step: 54, loss is 2.27152156829834\n",
      "epoch: 1 step: 55, loss is 2.22092866897583\n",
      "epoch: 1 step: 56, loss is 2.2282538414001465\n",
      "epoch: 1 step: 57, loss is 2.3149304389953613\n",
      "epoch: 1 step: 58, loss is 2.2818446159362793\n",
      "epoch: 1 step: 59, loss is 2.2450952529907227\n",
      "epoch: 1 step: 60, loss is 2.190884828567505\n",
      "epoch: 1 step: 61, loss is 2.226449489593506\n",
      "epoch: 1 step: 62, loss is 2.2169675827026367\n",
      "epoch: 1 step: 63, loss is 2.2906997203826904\n",
      "epoch: 1 step: 64, loss is 2.2817506790161133\n",
      "epoch: 1 step: 65, loss is 2.22992205619812\n",
      "epoch: 1 step: 66, loss is 2.291299819946289\n",
      "epoch: 1 step: 67, loss is 2.200723886489868\n",
      "epoch: 1 step: 68, loss is 2.22770094871521\n",
      "epoch: 1 step: 69, loss is 2.2335498332977295\n",
      "epoch: 1 step: 70, loss is 2.258302927017212\n",
      "epoch: 1 step: 71, loss is 2.227466344833374\n",
      "epoch: 1 step: 72, loss is 2.2792775630950928\n",
      "epoch: 1 step: 73, loss is 2.225476026535034\n",
      "epoch: 1 step: 74, loss is 2.237966775894165\n",
      "epoch: 1 step: 75, loss is 2.137146472930908\n",
      "epoch: 1 step: 76, loss is 2.2084028720855713\n",
      "epoch: 1 step: 77, loss is 2.228588104248047\n",
      "epoch: 1 step: 78, loss is 2.212616443634033\n",
      "epoch: 1 step: 79, loss is 2.1891849040985107\n",
      "epoch: 1 step: 80, loss is 2.2361905574798584\n",
      "epoch: 1 step: 81, loss is 2.25168514251709\n",
      "epoch: 1 step: 82, loss is 2.2548491954803467\n",
      "epoch: 1 step: 83, loss is 2.158926486968994\n",
      "epoch: 1 step: 84, loss is 2.265458345413208\n",
      "epoch: 1 step: 85, loss is 2.2511963844299316\n",
      "epoch: 1 step: 86, loss is 2.1327922344207764\n",
      "epoch: 1 step: 87, loss is 2.1956543922424316\n",
      "epoch: 1 step: 88, loss is 2.178694009780884\n",
      "epoch: 1 step: 89, loss is 2.133103370666504\n",
      "epoch: 1 step: 90, loss is 2.2173492908477783\n",
      "epoch: 1 step: 91, loss is 2.1555016040802\n",
      "epoch: 1 step: 92, loss is 2.197556734085083\n",
      "epoch: 1 step: 93, loss is 2.108840227127075\n",
      "epoch: 1 step: 94, loss is 2.0879156589508057\n",
      "epoch: 1 step: 95, loss is 2.17140531539917\n",
      "epoch: 1 step: 96, loss is 2.168760299682617\n",
      "epoch: 1 step: 97, loss is 2.1403510570526123\n",
      "epoch: 1 step: 98, loss is 2.1839582920074463\n",
      "epoch: 1 step: 99, loss is 2.27746844291687\n",
      "epoch: 1 step: 100, loss is 2.339907646179199\n",
      "epoch: 1 step: 101, loss is 2.110302209854126\n",
      "epoch: 1 step: 102, loss is 2.106356620788574\n",
      "epoch: 1 step: 103, loss is 2.1581871509552\n",
      "epoch: 1 step: 104, loss is 2.217714786529541\n",
      "epoch: 1 step: 105, loss is 2.1897060871124268\n",
      "epoch: 1 step: 106, loss is 2.192701578140259\n",
      "epoch: 1 step: 107, loss is 2.2463760375976562\n",
      "epoch: 1 step: 108, loss is 2.188194751739502\n",
      "epoch: 1 step: 109, loss is 2.211103677749634\n",
      "epoch: 1 step: 110, loss is 2.191401958465576\n",
      "epoch: 1 step: 111, loss is 2.2311909198760986\n",
      "epoch: 1 step: 112, loss is 2.2602648735046387\n",
      "epoch: 1 step: 113, loss is 2.3149335384368896\n",
      "epoch: 1 step: 114, loss is 2.0992519855499268\n",
      "epoch: 1 step: 115, loss is 2.2729854583740234\n",
      "epoch: 1 step: 116, loss is 2.2189319133758545\n",
      "epoch: 1 step: 117, loss is 2.225736618041992\n",
      "epoch: 1 step: 118, loss is 2.2816874980926514\n",
      "epoch: 1 step: 119, loss is 2.077462673187256\n",
      "epoch: 1 step: 120, loss is 2.115773916244507\n",
      "epoch: 1 step: 121, loss is 2.214648485183716\n",
      "epoch: 1 step: 122, loss is 2.1673779487609863\n",
      "epoch: 1 step: 123, loss is 2.1147589683532715\n",
      "epoch: 1 step: 124, loss is 2.183706045150757\n",
      "epoch: 1 step: 125, loss is 2.2864365577697754\n",
      "epoch: 1 step: 126, loss is 2.2032413482666016\n",
      "epoch: 1 step: 127, loss is 2.2483887672424316\n",
      "epoch: 1 step: 128, loss is 2.163769006729126\n",
      "epoch: 1 step: 129, loss is 2.1508071422576904\n",
      "epoch: 1 step: 130, loss is 2.237787961959839\n",
      "epoch: 1 step: 131, loss is 2.271061420440674\n",
      "epoch: 1 step: 132, loss is 2.22298002243042\n",
      "epoch: 1 step: 133, loss is 2.1225697994232178\n",
      "epoch: 1 step: 134, loss is 2.0566797256469727\n",
      "epoch: 1 step: 135, loss is 2.203519105911255\n",
      "epoch: 1 step: 136, loss is 2.179948329925537\n",
      "epoch: 1 step: 137, loss is 2.056015968322754\n",
      "epoch: 1 step: 138, loss is 2.086790084838867\n",
      "epoch: 1 step: 139, loss is 2.162785053253174\n",
      "epoch: 1 step: 140, loss is 2.1441116333007812\n",
      "epoch: 1 step: 141, loss is 2.132406711578369\n",
      "epoch: 1 step: 142, loss is 2.10667085647583\n",
      "epoch: 1 step: 143, loss is 2.1247239112854004\n",
      "epoch: 1 step: 144, loss is 2.083038568496704\n",
      "epoch: 1 step: 145, loss is 2.145554304122925\n",
      "epoch: 1 step: 146, loss is 2.1429574489593506\n",
      "epoch: 1 step: 147, loss is 2.1070945262908936\n",
      "epoch: 1 step: 148, loss is 2.210888385772705\n",
      "epoch: 1 step: 149, loss is 2.1297028064727783\n",
      "epoch: 1 step: 150, loss is 2.2372751235961914\n",
      "epoch: 1 step: 151, loss is 2.251737117767334\n",
      "epoch: 1 step: 152, loss is 2.0766658782958984\n",
      "epoch: 1 step: 153, loss is 2.047097682952881\n",
      "epoch: 1 step: 154, loss is 2.1337172985076904\n",
      "epoch: 1 step: 155, loss is 2.0538907051086426\n",
      "epoch: 1 step: 156, loss is 2.155696392059326\n",
      "epoch: 1 step: 157, loss is 2.056032657623291\n",
      "epoch: 1 step: 158, loss is 2.1752030849456787\n",
      "epoch: 1 step: 159, loss is 2.1970694065093994\n",
      "epoch: 1 step: 160, loss is 2.088735342025757\n",
      "epoch: 1 step: 161, loss is 2.0790815353393555\n",
      "epoch: 1 step: 162, loss is 2.115358591079712\n",
      "epoch: 1 step: 163, loss is 2.3544609546661377\n",
      "epoch: 1 step: 164, loss is 2.1790294647216797\n",
      "epoch: 1 step: 165, loss is 2.1789071559906006\n",
      "epoch: 1 step: 166, loss is 2.155616283416748\n",
      "epoch: 1 step: 167, loss is 2.03727126121521\n",
      "epoch: 1 step: 168, loss is 2.3169498443603516\n",
      "epoch: 1 step: 169, loss is 2.009615659713745\n",
      "epoch: 1 step: 170, loss is 2.250293016433716\n",
      "epoch: 1 step: 171, loss is 2.229249954223633\n",
      "epoch: 1 step: 172, loss is 2.1946017742156982\n",
      "epoch: 1 step: 173, loss is 2.106041669845581\n",
      "epoch: 1 step: 174, loss is 2.1328811645507812\n",
      "epoch: 1 step: 175, loss is 2.121337652206421\n",
      "epoch: 1 step: 176, loss is 2.048586368560791\n",
      "epoch: 1 step: 177, loss is 2.0093226432800293\n",
      "epoch: 1 step: 178, loss is 1.9809846878051758\n",
      "epoch: 1 step: 179, loss is 2.1085121631622314\n",
      "epoch: 1 step: 180, loss is 2.0339272022247314\n",
      "epoch: 1 step: 181, loss is 2.1930434703826904\n",
      "epoch: 1 step: 182, loss is 2.0522620677948\n",
      "epoch: 1 step: 183, loss is 2.082760810852051\n",
      "epoch: 1 step: 184, loss is 2.035794496536255\n",
      "epoch: 1 step: 185, loss is 2.0068657398223877\n",
      "epoch: 1 step: 186, loss is 2.1587584018707275\n",
      "epoch: 1 step: 187, loss is 2.1490421295166016\n",
      "epoch: 1 step: 188, loss is 2.183502435684204\n",
      "epoch: 1 step: 189, loss is 1.951444149017334\n",
      "epoch: 1 step: 190, loss is 2.233304262161255\n",
      "epoch: 1 step: 191, loss is 2.006812572479248\n",
      "epoch: 1 step: 192, loss is 1.9727572202682495\n",
      "epoch: 1 step: 193, loss is 2.1322827339172363\n",
      "epoch: 1 step: 194, loss is 2.222240447998047\n",
      "epoch: 1 step: 195, loss is 2.2365002632141113\n",
      "epoch: 1 step: 196, loss is 2.0550100803375244\n",
      "epoch: 1 step: 197, loss is 2.1314406394958496\n",
      "epoch: 1 step: 198, loss is 2.1344356536865234\n",
      "epoch: 1 step: 199, loss is 2.107867956161499\n",
      "epoch: 1 step: 200, loss is 1.9498319625854492\n",
      "epoch: 1 step: 201, loss is 2.1374850273132324\n",
      "epoch: 1 step: 202, loss is 2.2008402347564697\n",
      "epoch: 1 step: 203, loss is 2.1275360584259033\n",
      "epoch: 1 step: 204, loss is 2.271078109741211\n",
      "epoch: 1 step: 205, loss is 2.2204928398132324\n",
      "epoch: 1 step: 206, loss is 2.023313283920288\n",
      "epoch: 1 step: 207, loss is 2.0291664600372314\n",
      "epoch: 1 step: 208, loss is 2.0464868545532227\n",
      "epoch: 1 step: 209, loss is 2.197852611541748\n",
      "epoch: 1 step: 210, loss is 2.0805444717407227\n",
      "epoch: 1 step: 211, loss is 2.2118866443634033\n",
      "epoch: 1 step: 212, loss is 2.0345029830932617\n",
      "epoch: 1 step: 213, loss is 1.9367237091064453\n",
      "epoch: 1 step: 214, loss is 2.030815601348877\n",
      "epoch: 1 step: 215, loss is 2.1543283462524414\n",
      "epoch: 1 step: 216, loss is 2.2546255588531494\n",
      "epoch: 1 step: 217, loss is 2.1378345489501953\n",
      "epoch: 1 step: 218, loss is 2.1115036010742188\n",
      "epoch: 1 step: 219, loss is 2.0598208904266357\n",
      "epoch: 1 step: 220, loss is 2.0623233318328857\n",
      "epoch: 1 step: 221, loss is 2.137953758239746\n",
      "epoch: 1 step: 222, loss is 2.306215286254883\n",
      "epoch: 1 step: 223, loss is 2.0285096168518066\n",
      "epoch: 1 step: 224, loss is 2.057729721069336\n",
      "epoch: 1 step: 225, loss is 2.08978009223938\n",
      "epoch: 1 step: 226, loss is 2.1869497299194336\n",
      "epoch: 1 step: 227, loss is 2.095576524734497\n",
      "epoch: 1 step: 228, loss is 2.053562641143799\n",
      "epoch: 1 step: 229, loss is 2.2017416954040527\n",
      "epoch: 1 step: 230, loss is 2.0672712326049805\n",
      "epoch: 1 step: 231, loss is 2.0465950965881348\n",
      "epoch: 1 step: 232, loss is 2.032762050628662\n",
      "epoch: 1 step: 233, loss is 2.014310598373413\n",
      "epoch: 1 step: 234, loss is 2.038347005844116\n",
      "epoch: 1 step: 235, loss is 2.1739025115966797\n",
      "epoch: 1 step: 236, loss is 2.0226001739501953\n",
      "epoch: 1 step: 237, loss is 2.0863373279571533\n",
      "epoch: 1 step: 238, loss is 2.047053337097168\n",
      "epoch: 1 step: 239, loss is 2.0904953479766846\n",
      "epoch: 1 step: 240, loss is 1.971348524093628\n",
      "epoch: 1 step: 241, loss is 2.00530743598938\n",
      "epoch: 1 step: 242, loss is 2.2395758628845215\n",
      "epoch: 1 step: 243, loss is 2.0638630390167236\n",
      "epoch: 1 step: 244, loss is 2.2073771953582764\n",
      "epoch: 1 step: 245, loss is 2.3084280490875244\n",
      "epoch: 1 step: 246, loss is 2.072235345840454\n",
      "epoch: 1 step: 247, loss is 2.0667107105255127\n",
      "epoch: 1 step: 248, loss is 2.052905559539795\n",
      "epoch: 1 step: 249, loss is 2.037626266479492\n",
      "epoch: 1 step: 250, loss is 2.0511887073516846\n",
      "epoch: 1 step: 251, loss is 2.075904130935669\n",
      "epoch: 1 step: 252, loss is 2.178154706954956\n",
      "epoch: 1 step: 253, loss is 1.9204564094543457\n",
      "epoch: 1 step: 254, loss is 2.0513358116149902\n",
      "epoch: 1 step: 255, loss is 1.9931308031082153\n",
      "epoch: 1 step: 256, loss is 2.0522565841674805\n",
      "epoch: 1 step: 257, loss is 2.3253743648529053\n",
      "epoch: 1 step: 258, loss is 2.070582866668701\n",
      "epoch: 1 step: 259, loss is 2.0002849102020264\n",
      "epoch: 1 step: 260, loss is 2.2404115200042725\n",
      "epoch: 1 step: 261, loss is 2.0409164428710938\n",
      "epoch: 1 step: 262, loss is 2.2147979736328125\n",
      "epoch: 1 step: 263, loss is 2.0007429122924805\n",
      "epoch: 1 step: 264, loss is 2.1508243083953857\n",
      "epoch: 1 step: 265, loss is 2.179616928100586\n",
      "epoch: 1 step: 266, loss is 2.1073052883148193\n",
      "epoch: 1 step: 267, loss is 2.225925922393799\n",
      "epoch: 1 step: 268, loss is 1.9904991388320923\n",
      "epoch: 1 step: 269, loss is 1.907220721244812\n",
      "epoch: 1 step: 270, loss is 2.135770082473755\n",
      "epoch: 1 step: 271, loss is 2.009439468383789\n",
      "epoch: 1 step: 272, loss is 1.950438380241394\n",
      "epoch: 1 step: 273, loss is 1.9915307760238647\n",
      "epoch: 1 step: 274, loss is 2.3807170391082764\n",
      "epoch: 1 step: 275, loss is 2.0306811332702637\n",
      "epoch: 1 step: 276, loss is 2.183566093444824\n",
      "epoch: 1 step: 277, loss is 2.1794517040252686\n",
      "epoch: 1 step: 278, loss is 1.9860042333602905\n",
      "epoch: 1 step: 279, loss is 2.0349671840667725\n",
      "epoch: 1 step: 280, loss is 1.9700050354003906\n",
      "epoch: 1 step: 281, loss is 1.8756885528564453\n",
      "epoch: 1 step: 282, loss is 1.9880061149597168\n",
      "epoch: 1 step: 283, loss is 1.9853180646896362\n",
      "epoch: 1 step: 284, loss is 2.084918737411499\n",
      "epoch: 1 step: 285, loss is 2.095715284347534\n",
      "epoch: 1 step: 286, loss is 2.185358762741089\n",
      "epoch: 1 step: 287, loss is 2.035236358642578\n",
      "epoch: 1 step: 288, loss is 1.99566650390625\n",
      "epoch: 1 step: 289, loss is 1.9636954069137573\n",
      "epoch: 1 step: 290, loss is 2.0333714485168457\n",
      "epoch: 1 step: 291, loss is 1.9071580171585083\n",
      "epoch: 1 step: 292, loss is 1.977942705154419\n",
      "epoch: 1 step: 293, loss is 2.0269320011138916\n",
      "epoch: 1 step: 294, loss is 2.007492780685425\n",
      "epoch: 1 step: 295, loss is 2.2547740936279297\n",
      "epoch: 1 step: 296, loss is 2.040421962738037\n",
      "epoch: 1 step: 297, loss is 2.1423146724700928\n",
      "epoch: 1 step: 298, loss is 2.061828374862671\n",
      "epoch: 1 step: 299, loss is 1.9716166257858276\n",
      "epoch: 1 step: 300, loss is 2.055492877960205\n",
      "epoch: 1 step: 301, loss is 2.18951678276062\n",
      "epoch: 1 step: 302, loss is 2.059842348098755\n",
      "epoch: 1 step: 303, loss is 1.8466212749481201\n",
      "epoch: 1 step: 304, loss is 2.113541603088379\n",
      "epoch: 1 step: 305, loss is 2.054375410079956\n",
      "epoch: 1 step: 306, loss is 1.9479498863220215\n",
      "epoch: 1 step: 307, loss is 2.089711904525757\n",
      "epoch: 1 step: 308, loss is 2.1464731693267822\n",
      "epoch: 1 step: 309, loss is 2.0618953704833984\n",
      "epoch: 1 step: 310, loss is 1.9916982650756836\n",
      "epoch: 1 step: 311, loss is 1.9203888177871704\n",
      "epoch: 1 step: 312, loss is 2.0564444065093994\n",
      "epoch: 1 step: 313, loss is 1.9224743843078613\n",
      "epoch: 1 step: 314, loss is 1.905186414718628\n",
      "epoch: 1 step: 315, loss is 1.9768773317337036\n",
      "epoch: 1 step: 316, loss is 2.1241023540496826\n",
      "epoch: 1 step: 317, loss is 1.995172381401062\n",
      "epoch: 1 step: 318, loss is 2.035067081451416\n",
      "epoch: 1 step: 319, loss is 2.134002447128296\n",
      "epoch: 1 step: 320, loss is 1.958056926727295\n",
      "epoch: 1 step: 321, loss is 2.061570405960083\n",
      "epoch: 1 step: 322, loss is 2.017692804336548\n",
      "epoch: 1 step: 323, loss is 2.203303337097168\n",
      "epoch: 1 step: 324, loss is 1.8988637924194336\n",
      "epoch: 1 step: 325, loss is 2.0121819972991943\n",
      "epoch: 1 step: 326, loss is 1.934653878211975\n",
      "epoch: 1 step: 327, loss is 1.9111517667770386\n",
      "epoch: 1 step: 328, loss is 2.036140203475952\n",
      "epoch: 1 step: 329, loss is 2.0087573528289795\n",
      "epoch: 1 step: 330, loss is 2.156726837158203\n",
      "epoch: 1 step: 331, loss is 2.0427544116973877\n",
      "epoch: 1 step: 332, loss is 2.0969061851501465\n",
      "epoch: 1 step: 333, loss is 2.0413706302642822\n",
      "epoch: 1 step: 334, loss is 2.0374600887298584\n",
      "epoch: 1 step: 335, loss is 1.8641756772994995\n",
      "epoch: 1 step: 336, loss is 1.901416540145874\n",
      "epoch: 1 step: 337, loss is 2.0092039108276367\n",
      "epoch: 1 step: 338, loss is 2.06730580329895\n",
      "epoch: 1 step: 339, loss is 2.18129825592041\n",
      "epoch: 1 step: 340, loss is 2.218785047531128\n",
      "epoch: 1 step: 341, loss is 2.1022849082946777\n",
      "epoch: 1 step: 342, loss is 1.9576345682144165\n",
      "epoch: 1 step: 343, loss is 1.894235372543335\n",
      "epoch: 1 step: 344, loss is 1.9741086959838867\n",
      "epoch: 1 step: 345, loss is 2.099330425262451\n",
      "epoch: 1 step: 346, loss is 2.3195230960845947\n",
      "epoch: 1 step: 347, loss is 2.1057145595550537\n",
      "epoch: 1 step: 348, loss is 1.947761058807373\n",
      "epoch: 1 step: 349, loss is 2.0613021850585938\n",
      "epoch: 1 step: 350, loss is 2.1159989833831787\n",
      "epoch: 1 step: 351, loss is 2.0741612911224365\n",
      "epoch: 1 step: 352, loss is 2.0469584465026855\n",
      "epoch: 1 step: 353, loss is 2.0399539470672607\n",
      "epoch: 1 step: 354, loss is 2.0152463912963867\n",
      "epoch: 1 step: 355, loss is 1.965977430343628\n",
      "epoch: 1 step: 356, loss is 2.0025906562805176\n",
      "epoch: 1 step: 357, loss is 2.272578239440918\n",
      "epoch: 1 step: 358, loss is 2.0018327236175537\n",
      "epoch: 1 step: 359, loss is 2.0626683235168457\n",
      "epoch: 1 step: 360, loss is 2.007896661758423\n",
      "epoch: 1 step: 361, loss is 2.030329465866089\n",
      "epoch: 1 step: 362, loss is 2.0910191535949707\n",
      "epoch: 1 step: 363, loss is 2.1232550144195557\n",
      "epoch: 1 step: 364, loss is 1.9034137725830078\n",
      "epoch: 1 step: 365, loss is 1.9970991611480713\n",
      "epoch: 1 step: 366, loss is 2.059720039367676\n",
      "epoch: 1 step: 367, loss is 1.9302160739898682\n",
      "epoch: 1 step: 368, loss is 1.953922152519226\n",
      "epoch: 1 step: 369, loss is 2.0171968936920166\n",
      "epoch: 1 step: 370, loss is 2.023792028427124\n",
      "epoch: 1 step: 371, loss is 2.1135401725769043\n",
      "epoch: 1 step: 372, loss is 1.9883006811141968\n",
      "epoch: 1 step: 373, loss is 2.291741371154785\n",
      "epoch: 1 step: 374, loss is 1.9816128015518188\n",
      "epoch: 1 step: 375, loss is 2.0700929164886475\n",
      "epoch: 1 step: 376, loss is 1.751235008239746\n",
      "epoch: 1 step: 377, loss is 1.9834214448928833\n",
      "epoch: 1 step: 378, loss is 2.0533463954925537\n",
      "epoch: 1 step: 379, loss is 1.919065237045288\n",
      "epoch: 1 step: 380, loss is 1.806645154953003\n",
      "epoch: 1 step: 381, loss is 2.0177924633026123\n",
      "epoch: 1 step: 382, loss is 1.988266110420227\n",
      "epoch: 1 step: 383, loss is 2.086853265762329\n",
      "epoch: 1 step: 384, loss is 1.8515771627426147\n",
      "epoch: 1 step: 385, loss is 1.842163324356079\n",
      "epoch: 1 step: 386, loss is 2.029099702835083\n",
      "epoch: 1 step: 387, loss is 2.2015395164489746\n",
      "epoch: 1 step: 388, loss is 1.909118413925171\n",
      "epoch: 1 step: 389, loss is 2.1463239192962646\n",
      "epoch: 1 step: 390, loss is 2.0062599182128906\n",
      "epoch: 1 step: 391, loss is 2.076389789581299\n",
      "epoch: 1 step: 392, loss is 2.12922739982605\n",
      "epoch: 1 step: 393, loss is 1.9859113693237305\n",
      "epoch: 1 step: 394, loss is 2.184828996658325\n",
      "epoch: 1 step: 395, loss is 2.1131038665771484\n",
      "epoch: 1 step: 396, loss is 1.986657738685608\n",
      "epoch: 1 step: 397, loss is 2.013418197631836\n",
      "epoch: 1 step: 398, loss is 2.111126661300659\n",
      "epoch: 1 step: 399, loss is 2.0266261100769043\n",
      "epoch: 1 step: 400, loss is 2.0410478115081787\n",
      "epoch: 1 step: 401, loss is 2.0155534744262695\n",
      "epoch: 1 step: 402, loss is 2.0019710063934326\n",
      "epoch: 1 step: 403, loss is 1.8749558925628662\n",
      "epoch: 1 step: 404, loss is 1.9068533182144165\n",
      "epoch: 1 step: 405, loss is 2.1750802993774414\n",
      "epoch: 1 step: 406, loss is 1.981999397277832\n",
      "epoch: 1 step: 407, loss is 2.2525761127471924\n",
      "epoch: 1 step: 408, loss is 2.0596330165863037\n",
      "epoch: 1 step: 409, loss is 2.1841304302215576\n",
      "epoch: 1 step: 410, loss is 2.166287899017334\n",
      "epoch: 1 step: 411, loss is 1.8855327367782593\n",
      "epoch: 1 step: 412, loss is 2.011155366897583\n",
      "epoch: 1 step: 413, loss is 2.098884344100952\n",
      "epoch: 1 step: 414, loss is 1.9338797330856323\n",
      "epoch: 1 step: 415, loss is 1.836780309677124\n",
      "epoch: 1 step: 416, loss is 1.9226139783859253\n",
      "epoch: 1 step: 417, loss is 2.116323232650757\n",
      "epoch: 1 step: 418, loss is 1.9762972593307495\n",
      "epoch: 1 step: 419, loss is 1.9583468437194824\n",
      "epoch: 1 step: 420, loss is 2.0269858837127686\n",
      "epoch: 1 step: 421, loss is 2.154264211654663\n",
      "epoch: 1 step: 422, loss is 1.9174524545669556\n",
      "epoch: 1 step: 423, loss is 2.0104727745056152\n",
      "epoch: 1 step: 424, loss is 1.9695701599121094\n",
      "epoch: 1 step: 425, loss is 2.0990920066833496\n",
      "epoch: 1 step: 426, loss is 1.8161929845809937\n",
      "epoch: 1 step: 427, loss is 2.0785388946533203\n",
      "epoch: 1 step: 428, loss is 1.9426262378692627\n",
      "epoch: 1 step: 429, loss is 2.098062038421631\n",
      "epoch: 1 step: 430, loss is 1.9641289710998535\n",
      "epoch: 1 step: 431, loss is 2.1740338802337646\n",
      "epoch: 1 step: 432, loss is 2.0240070819854736\n",
      "epoch: 1 step: 433, loss is 2.0075082778930664\n",
      "epoch: 1 step: 434, loss is 1.881856083869934\n",
      "epoch: 1 step: 435, loss is 2.2026638984680176\n",
      "epoch: 1 step: 436, loss is 1.985649585723877\n",
      "epoch: 1 step: 437, loss is 1.9341983795166016\n",
      "epoch: 1 step: 438, loss is 2.0520670413970947\n",
      "epoch: 1 step: 439, loss is 2.0158276557922363\n",
      "epoch: 1 step: 440, loss is 1.883876085281372\n",
      "epoch: 1 step: 441, loss is 2.044785976409912\n",
      "epoch: 1 step: 442, loss is 2.0058350563049316\n",
      "epoch: 1 step: 443, loss is 1.998342752456665\n",
      "epoch: 1 step: 444, loss is 2.172030210494995\n",
      "epoch: 1 step: 445, loss is 1.9234371185302734\n",
      "epoch: 1 step: 446, loss is 1.892642855644226\n",
      "epoch: 1 step: 447, loss is 2.158596992492676\n",
      "epoch: 1 step: 448, loss is 1.9201840162277222\n",
      "epoch: 1 step: 449, loss is 1.8731005191802979\n",
      "epoch: 1 step: 450, loss is 1.877543330192566\n",
      "epoch: 1 step: 451, loss is 1.9526846408843994\n",
      "epoch: 1 step: 452, loss is 2.0912270545959473\n",
      "epoch: 1 step: 453, loss is 1.9741483926773071\n",
      "epoch: 1 step: 454, loss is 1.9220128059387207\n",
      "epoch: 1 step: 455, loss is 2.07645583152771\n",
      "epoch: 1 step: 456, loss is 2.068852663040161\n",
      "epoch: 1 step: 457, loss is 2.144120216369629\n",
      "epoch: 1 step: 458, loss is 2.165879964828491\n",
      "epoch: 1 step: 459, loss is 1.8409161567687988\n",
      "epoch: 1 step: 460, loss is 1.925309181213379\n",
      "epoch: 1 step: 461, loss is 1.9597374200820923\n",
      "epoch: 1 step: 462, loss is 1.8873956203460693\n",
      "epoch: 1 step: 463, loss is 1.9186104536056519\n",
      "epoch: 1 step: 464, loss is 2.1392109394073486\n",
      "epoch: 1 step: 465, loss is 1.9498599767684937\n",
      "epoch: 1 step: 466, loss is 1.9833217859268188\n",
      "epoch: 1 step: 467, loss is 1.8746776580810547\n",
      "epoch: 1 step: 468, loss is 1.8127049207687378\n",
      "epoch: 1 step: 469, loss is 1.952332854270935\n",
      "epoch: 1 step: 470, loss is 2.0186619758605957\n",
      "epoch: 1 step: 471, loss is 1.88279390335083\n",
      "epoch: 1 step: 472, loss is 1.8357880115509033\n",
      "epoch: 1 step: 473, loss is 1.980440616607666\n",
      "epoch: 1 step: 474, loss is 2.2762510776519775\n",
      "epoch: 1 step: 475, loss is 2.010420322418213\n",
      "epoch: 1 step: 476, loss is 1.87736976146698\n",
      "epoch: 1 step: 477, loss is 1.9180508852005005\n",
      "epoch: 1 step: 478, loss is 2.0709238052368164\n",
      "epoch: 1 step: 479, loss is 2.130676746368408\n",
      "epoch: 1 step: 480, loss is 1.887251377105713\n",
      "epoch: 1 step: 481, loss is 1.975150227546692\n",
      "epoch: 1 step: 482, loss is 1.8476405143737793\n",
      "epoch: 1 step: 483, loss is 1.7960580587387085\n",
      "epoch: 1 step: 484, loss is 1.9182215929031372\n",
      "epoch: 1 step: 485, loss is 1.9044901132583618\n",
      "epoch: 1 step: 486, loss is 1.9769495725631714\n",
      "epoch: 1 step: 487, loss is 2.094264507293701\n",
      "epoch: 1 step: 488, loss is 1.7241907119750977\n",
      "epoch: 1 step: 489, loss is 2.0850460529327393\n",
      "epoch: 1 step: 490, loss is 2.04288387298584\n",
      "epoch: 1 step: 491, loss is 1.8806148767471313\n",
      "epoch: 1 step: 492, loss is 1.8673239946365356\n",
      "epoch: 1 step: 493, loss is 2.037869930267334\n",
      "epoch: 1 step: 494, loss is 2.2135660648345947\n",
      "epoch: 1 step: 495, loss is 1.9971894025802612\n",
      "epoch: 1 step: 496, loss is 1.808595895767212\n",
      "epoch: 1 step: 497, loss is 1.884753942489624\n",
      "epoch: 1 step: 498, loss is 2.0341851711273193\n",
      "epoch: 1 step: 499, loss is 1.95365571975708\n",
      "epoch: 1 step: 500, loss is 1.7794464826583862\n",
      "epoch: 1 step: 501, loss is 2.0189719200134277\n",
      "epoch: 1 step: 502, loss is 1.8650211095809937\n",
      "epoch: 1 step: 503, loss is 1.9503087997436523\n",
      "epoch: 1 step: 504, loss is 2.1645865440368652\n",
      "epoch: 1 step: 505, loss is 1.7112220525741577\n",
      "epoch: 1 step: 506, loss is 2.0499765872955322\n",
      "epoch: 1 step: 507, loss is 2.133359670639038\n",
      "epoch: 1 step: 508, loss is 2.1380255222320557\n",
      "epoch: 1 step: 509, loss is 1.8259648084640503\n",
      "epoch: 1 step: 510, loss is 1.905226230621338\n",
      "epoch: 1 step: 511, loss is 2.192432403564453\n",
      "epoch: 1 step: 512, loss is 1.8600859642028809\n",
      "epoch: 1 step: 513, loss is 2.0558152198791504\n",
      "epoch: 1 step: 514, loss is 1.9997804164886475\n",
      "epoch: 1 step: 515, loss is 2.013768434524536\n",
      "epoch: 1 step: 516, loss is 2.039066791534424\n",
      "epoch: 1 step: 517, loss is 1.9810408353805542\n",
      "epoch: 1 step: 518, loss is 1.716402530670166\n",
      "epoch: 1 step: 519, loss is 1.9299750328063965\n",
      "epoch: 1 step: 520, loss is 2.041930675506592\n",
      "epoch: 1 step: 521, loss is 2.0946707725524902\n",
      "epoch: 1 step: 522, loss is 1.895356297492981\n",
      "epoch: 1 step: 523, loss is 2.2532389163970947\n",
      "epoch: 1 step: 524, loss is 1.9455857276916504\n",
      "epoch: 1 step: 525, loss is 1.8682266473770142\n",
      "epoch: 1 step: 526, loss is 1.936318278312683\n",
      "epoch: 1 step: 527, loss is 2.161334991455078\n",
      "epoch: 1 step: 528, loss is 2.0688652992248535\n",
      "epoch: 1 step: 529, loss is 1.9602992534637451\n",
      "epoch: 1 step: 530, loss is 1.8774570226669312\n",
      "epoch: 1 step: 531, loss is 2.150021553039551\n",
      "epoch: 1 step: 532, loss is 1.9236551523208618\n",
      "epoch: 1 step: 533, loss is 2.0080173015594482\n",
      "epoch: 1 step: 534, loss is 2.300452470779419\n",
      "epoch: 1 step: 535, loss is 1.817681074142456\n",
      "epoch: 1 step: 536, loss is 1.7938587665557861\n",
      "epoch: 1 step: 537, loss is 2.057171106338501\n",
      "epoch: 1 step: 538, loss is 1.8683322668075562\n",
      "epoch: 1 step: 539, loss is 2.07658314704895\n",
      "epoch: 1 step: 540, loss is 2.3469929695129395\n",
      "epoch: 1 step: 541, loss is 1.7835749387741089\n",
      "epoch: 1 step: 542, loss is 1.9507569074630737\n",
      "epoch: 1 step: 543, loss is 1.796126127243042\n",
      "epoch: 1 step: 544, loss is 1.978717565536499\n",
      "epoch: 1 step: 545, loss is 1.7953993082046509\n",
      "epoch: 1 step: 546, loss is 2.0468051433563232\n",
      "epoch: 1 step: 547, loss is 1.8574296236038208\n",
      "epoch: 1 step: 548, loss is 1.900364637374878\n",
      "epoch: 1 step: 549, loss is 1.8828262090682983\n",
      "epoch: 1 step: 550, loss is 1.918333649635315\n",
      "epoch: 1 step: 551, loss is 1.9300329685211182\n",
      "epoch: 1 step: 552, loss is 1.8423584699630737\n",
      "epoch: 1 step: 553, loss is 2.1562390327453613\n",
      "epoch: 1 step: 554, loss is 1.9614213705062866\n",
      "epoch: 1 step: 555, loss is 1.8318755626678467\n",
      "epoch: 1 step: 556, loss is 1.899804711341858\n",
      "epoch: 1 step: 557, loss is 2.0100715160369873\n",
      "epoch: 1 step: 558, loss is 1.876407504081726\n",
      "epoch: 1 step: 559, loss is 1.95480477809906\n",
      "epoch: 1 step: 560, loss is 2.2042605876922607\n",
      "epoch: 1 step: 561, loss is 1.7937473058700562\n",
      "epoch: 1 step: 562, loss is 1.9287452697753906\n",
      "epoch: 1 step: 563, loss is 1.6975566148757935\n",
      "epoch: 1 step: 564, loss is 1.9455686807632446\n",
      "epoch: 1 step: 565, loss is 1.8215575218200684\n",
      "epoch: 1 step: 566, loss is 1.7477209568023682\n",
      "epoch: 1 step: 567, loss is 1.7751613855361938\n",
      "epoch: 1 step: 568, loss is 1.833632469177246\n",
      "epoch: 1 step: 569, loss is 1.7615571022033691\n",
      "epoch: 1 step: 570, loss is 1.974754810333252\n",
      "epoch: 1 step: 571, loss is 2.176231622695923\n",
      "epoch: 1 step: 572, loss is 2.0042049884796143\n",
      "epoch: 1 step: 573, loss is 2.074580669403076\n",
      "epoch: 1 step: 574, loss is 1.8660932779312134\n",
      "epoch: 1 step: 575, loss is 1.903387188911438\n",
      "epoch: 1 step: 576, loss is 2.1204802989959717\n",
      "epoch: 1 step: 577, loss is 2.119058847427368\n",
      "epoch: 1 step: 578, loss is 1.9040859937667847\n",
      "epoch: 1 step: 579, loss is 2.0645925998687744\n",
      "epoch: 1 step: 580, loss is 1.8789606094360352\n",
      "epoch: 1 step: 581, loss is 2.143294095993042\n",
      "epoch: 1 step: 582, loss is 2.2880687713623047\n",
      "epoch: 1 step: 583, loss is 1.8272855281829834\n",
      "epoch: 1 step: 584, loss is 1.9233195781707764\n",
      "epoch: 1 step: 585, loss is 2.0592005252838135\n",
      "epoch: 1 step: 586, loss is 1.904152274131775\n",
      "epoch: 1 step: 587, loss is 1.8236483335494995\n",
      "epoch: 1 step: 588, loss is 2.120938301086426\n",
      "epoch: 1 step: 589, loss is 1.8477091789245605\n",
      "epoch: 1 step: 590, loss is 1.9534707069396973\n",
      "epoch: 1 step: 591, loss is 1.9051223993301392\n",
      "epoch: 1 step: 592, loss is 2.096843719482422\n",
      "epoch: 1 step: 593, loss is 2.0139646530151367\n",
      "epoch: 1 step: 594, loss is 1.9648377895355225\n",
      "epoch: 1 step: 595, loss is 2.0036308765411377\n",
      "epoch: 1 step: 596, loss is 1.8923897743225098\n",
      "epoch: 1 step: 597, loss is 1.8687037229537964\n",
      "epoch: 1 step: 598, loss is 2.0104753971099854\n",
      "epoch: 1 step: 599, loss is 1.9063307046890259\n",
      "epoch: 1 step: 600, loss is 2.0246570110321045\n",
      "epoch: 1 step: 601, loss is 1.8029361963272095\n",
      "epoch: 1 step: 602, loss is 1.9748390913009644\n",
      "epoch: 1 step: 603, loss is 1.8419620990753174\n",
      "epoch: 1 step: 604, loss is 1.9773410558700562\n",
      "epoch: 1 step: 605, loss is 1.9168457984924316\n",
      "epoch: 1 step: 606, loss is 1.9091696739196777\n",
      "epoch: 1 step: 607, loss is 1.8956776857376099\n",
      "epoch: 1 step: 608, loss is 1.9233752489089966\n",
      "epoch: 1 step: 609, loss is 1.9893383979797363\n",
      "epoch: 1 step: 610, loss is 2.0830318927764893\n",
      "epoch: 1 step: 611, loss is 1.9757241010665894\n",
      "epoch: 1 step: 612, loss is 1.8929136991500854\n",
      "epoch: 1 step: 613, loss is 1.9042304754257202\n",
      "epoch: 1 step: 614, loss is 1.8328144550323486\n",
      "epoch: 1 step: 615, loss is 2.103163957595825\n",
      "epoch: 1 step: 616, loss is 1.8254390954971313\n",
      "epoch: 1 step: 617, loss is 2.12357759475708\n",
      "epoch: 1 step: 618, loss is 1.9225136041641235\n",
      "epoch: 1 step: 619, loss is 2.1116209030151367\n",
      "epoch: 1 step: 620, loss is 1.8749754428863525\n",
      "epoch: 1 step: 621, loss is 2.1287670135498047\n",
      "epoch: 1 step: 622, loss is 1.7956795692443848\n",
      "epoch: 1 step: 623, loss is 1.9082000255584717\n",
      "epoch: 1 step: 624, loss is 2.0220770835876465\n",
      "epoch: 1 step: 625, loss is 1.934491515159607\n",
      "epoch: 1 step: 626, loss is 1.6699644327163696\n",
      "epoch: 1 step: 627, loss is 2.07328200340271\n",
      "epoch: 1 step: 628, loss is 1.934311866760254\n",
      "epoch: 1 step: 629, loss is 1.8822872638702393\n",
      "epoch: 1 step: 630, loss is 1.980325698852539\n",
      "epoch: 1 step: 631, loss is 1.8497288227081299\n",
      "epoch: 1 step: 632, loss is 1.8306610584259033\n",
      "epoch: 1 step: 633, loss is 1.8633753061294556\n",
      "epoch: 1 step: 634, loss is 1.6261855363845825\n",
      "epoch: 1 step: 635, loss is 1.751717209815979\n",
      "epoch: 1 step: 636, loss is 1.9441275596618652\n",
      "epoch: 1 step: 637, loss is 1.9656577110290527\n",
      "epoch: 1 step: 638, loss is 2.1198437213897705\n",
      "epoch: 1 step: 639, loss is 1.9888311624526978\n",
      "epoch: 1 step: 640, loss is 1.7117822170257568\n",
      "epoch: 1 step: 641, loss is 2.0332303047180176\n",
      "epoch: 1 step: 642, loss is 1.8361821174621582\n",
      "epoch: 1 step: 643, loss is 2.0806946754455566\n",
      "epoch: 1 step: 644, loss is 1.8790125846862793\n",
      "epoch: 1 step: 645, loss is 1.930802583694458\n",
      "epoch: 1 step: 646, loss is 1.8917325735092163\n",
      "epoch: 1 step: 647, loss is 1.8431752920150757\n",
      "epoch: 1 step: 648, loss is 1.7952057123184204\n",
      "epoch: 1 step: 649, loss is 1.9666881561279297\n",
      "epoch: 1 step: 650, loss is 1.8641202449798584\n",
      "epoch: 1 step: 651, loss is 1.9653849601745605\n",
      "epoch: 1 step: 652, loss is 1.7072889804840088\n",
      "epoch: 1 step: 653, loss is 1.8962523937225342\n",
      "epoch: 1 step: 654, loss is 1.7344521284103394\n",
      "epoch: 1 step: 655, loss is 2.021075487136841\n",
      "epoch: 1 step: 656, loss is 2.029639959335327\n",
      "epoch: 1 step: 657, loss is 1.738716959953308\n",
      "epoch: 1 step: 658, loss is 1.9662022590637207\n",
      "epoch: 1 step: 659, loss is 1.8585591316223145\n",
      "epoch: 1 step: 660, loss is 1.9980499744415283\n",
      "epoch: 1 step: 661, loss is 1.8533803224563599\n",
      "epoch: 1 step: 662, loss is 1.786396861076355\n",
      "epoch: 1 step: 663, loss is 1.8964468240737915\n",
      "epoch: 1 step: 664, loss is 1.7689759731292725\n",
      "epoch: 1 step: 665, loss is 1.778600811958313\n",
      "epoch: 1 step: 666, loss is 1.8252822160720825\n",
      "epoch: 1 step: 667, loss is 2.097815752029419\n",
      "epoch: 1 step: 668, loss is 1.9108268022537231\n",
      "epoch: 1 step: 669, loss is 2.1424691677093506\n",
      "epoch: 1 step: 670, loss is 1.9559458494186401\n",
      "epoch: 1 step: 671, loss is 2.231980800628662\n",
      "epoch: 1 step: 672, loss is 2.0064918994903564\n",
      "epoch: 1 step: 673, loss is 1.855647325515747\n",
      "epoch: 1 step: 674, loss is 1.9518969058990479\n",
      "epoch: 1 step: 675, loss is 1.8046430349349976\n",
      "epoch: 1 step: 676, loss is 1.8244946002960205\n",
      "epoch: 1 step: 677, loss is 1.9626678228378296\n",
      "epoch: 1 step: 678, loss is 2.037781000137329\n",
      "epoch: 1 step: 679, loss is 1.8904790878295898\n",
      "epoch: 1 step: 680, loss is 1.9447929859161377\n",
      "epoch: 1 step: 681, loss is 1.8932394981384277\n",
      "epoch: 1 step: 682, loss is 1.8192253112792969\n",
      "epoch: 1 step: 683, loss is 2.229520320892334\n",
      "epoch: 1 step: 684, loss is 1.8193644285202026\n",
      "epoch: 1 step: 685, loss is 2.011110544204712\n",
      "epoch: 1 step: 686, loss is 1.7468148469924927\n",
      "epoch: 1 step: 687, loss is 1.9464117288589478\n",
      "epoch: 1 step: 688, loss is 2.1069817543029785\n",
      "epoch: 1 step: 689, loss is 1.7781250476837158\n",
      "epoch: 1 step: 690, loss is 1.76134192943573\n",
      "epoch: 1 step: 691, loss is 1.863200306892395\n",
      "epoch: 1 step: 692, loss is 1.758194923400879\n",
      "epoch: 1 step: 693, loss is 1.8789228200912476\n",
      "epoch: 1 step: 694, loss is 2.0136072635650635\n",
      "epoch: 1 step: 695, loss is 1.749578595161438\n",
      "epoch: 1 step: 696, loss is 2.009148597717285\n",
      "epoch: 1 step: 697, loss is 1.7434495687484741\n",
      "epoch: 1 step: 698, loss is 1.8487495183944702\n",
      "epoch: 1 step: 699, loss is 1.9375704526901245\n",
      "epoch: 1 step: 700, loss is 2.027942657470703\n",
      "epoch: 1 step: 701, loss is 1.821093201637268\n",
      "epoch: 1 step: 702, loss is 2.159715414047241\n",
      "epoch: 1 step: 703, loss is 1.8883578777313232\n",
      "epoch: 1 step: 704, loss is 1.8912168741226196\n",
      "epoch: 1 step: 705, loss is 1.7937527894973755\n",
      "epoch: 1 step: 706, loss is 1.9539607763290405\n",
      "epoch: 1 step: 707, loss is 2.057384729385376\n",
      "epoch: 1 step: 708, loss is 1.9515490531921387\n",
      "epoch: 1 step: 709, loss is 1.7827140092849731\n",
      "epoch: 1 step: 710, loss is 1.7843291759490967\n",
      "epoch: 1 step: 711, loss is 2.203721284866333\n",
      "epoch: 1 step: 712, loss is 1.936421275138855\n",
      "epoch: 1 step: 713, loss is 2.212958574295044\n",
      "epoch: 1 step: 714, loss is 1.9564666748046875\n",
      "epoch: 1 step: 715, loss is 1.9245672225952148\n",
      "epoch: 1 step: 716, loss is 1.8904215097427368\n",
      "epoch: 1 step: 717, loss is 2.20047926902771\n",
      "epoch: 1 step: 718, loss is 1.9269334077835083\n",
      "epoch: 1 step: 719, loss is 1.9670847654342651\n",
      "epoch: 1 step: 720, loss is 1.6823962926864624\n",
      "epoch: 1 step: 721, loss is 1.9217621088027954\n",
      "epoch: 1 step: 722, loss is 1.8484920263290405\n",
      "epoch: 1 step: 723, loss is 1.9388822317123413\n",
      "epoch: 1 step: 724, loss is 1.856508731842041\n",
      "epoch: 1 step: 725, loss is 2.1508233547210693\n",
      "epoch: 1 step: 726, loss is 1.8139601945877075\n",
      "epoch: 1 step: 727, loss is 1.9360324144363403\n",
      "epoch: 1 step: 728, loss is 1.933687448501587\n",
      "epoch: 1 step: 729, loss is 1.61446213722229\n",
      "epoch: 1 step: 730, loss is 1.8012168407440186\n",
      "epoch: 1 step: 731, loss is 1.9122138023376465\n",
      "epoch: 1 step: 732, loss is 1.736993670463562\n",
      "epoch: 1 step: 733, loss is 1.9705088138580322\n",
      "epoch: 1 step: 734, loss is 2.0239226818084717\n",
      "epoch: 1 step: 735, loss is 1.84898042678833\n",
      "epoch: 1 step: 736, loss is 1.817182183265686\n",
      "epoch: 1 step: 737, loss is 1.9165890216827393\n",
      "epoch: 1 step: 738, loss is 1.934903860092163\n",
      "epoch: 1 step: 739, loss is 1.9177310466766357\n",
      "epoch: 1 step: 740, loss is 1.880964756011963\n",
      "epoch: 1 step: 741, loss is 1.8801833391189575\n",
      "epoch: 1 step: 742, loss is 1.9676061868667603\n",
      "epoch: 1 step: 743, loss is 1.848497986793518\n",
      "epoch: 1 step: 744, loss is 2.0522735118865967\n",
      "epoch: 1 step: 745, loss is 1.9187451601028442\n",
      "epoch: 1 step: 746, loss is 1.8972903490066528\n",
      "epoch: 1 step: 747, loss is 1.9079883098602295\n",
      "epoch: 1 step: 748, loss is 2.089838981628418\n",
      "epoch: 1 step: 749, loss is 1.9596408605575562\n",
      "epoch: 1 step: 750, loss is 1.7652751207351685\n",
      "epoch: 1 step: 751, loss is 1.714115858078003\n",
      "epoch: 1 step: 752, loss is 1.7904343605041504\n",
      "epoch: 1 step: 753, loss is 1.7457854747772217\n",
      "epoch: 1 step: 754, loss is 2.0732686519622803\n",
      "epoch: 1 step: 755, loss is 1.7087560892105103\n",
      "epoch: 1 step: 756, loss is 1.774334192276001\n",
      "epoch: 1 step: 757, loss is 2.1757211685180664\n",
      "epoch: 1 step: 758, loss is 1.8479453325271606\n",
      "epoch: 1 step: 759, loss is 1.5710399150848389\n",
      "epoch: 1 step: 760, loss is 1.9513400793075562\n",
      "epoch: 1 step: 761, loss is 1.7228890657424927\n",
      "epoch: 1 step: 762, loss is 2.1457061767578125\n",
      "epoch: 1 step: 763, loss is 1.8098945617675781\n",
      "epoch: 1 step: 764, loss is 2.0317044258117676\n",
      "epoch: 1 step: 765, loss is 1.8087273836135864\n",
      "epoch: 1 step: 766, loss is 2.0938735008239746\n",
      "epoch: 1 step: 767, loss is 1.8029885292053223\n",
      "epoch: 1 step: 768, loss is 1.834389090538025\n",
      "epoch: 1 step: 769, loss is 1.7395254373550415\n",
      "epoch: 1 step: 770, loss is 2.048811197280884\n",
      "epoch: 1 step: 771, loss is 1.6545623540878296\n",
      "epoch: 1 step: 772, loss is 1.7641065120697021\n",
      "epoch: 1 step: 773, loss is 1.7573646306991577\n",
      "epoch: 1 step: 774, loss is 1.9882786273956299\n",
      "epoch: 1 step: 775, loss is 1.8003183603286743\n",
      "epoch: 1 step: 776, loss is 1.745949149131775\n",
      "epoch: 1 step: 777, loss is 1.912627935409546\n",
      "epoch: 1 step: 778, loss is 1.8348170518875122\n",
      "epoch: 1 step: 779, loss is 1.719604253768921\n",
      "epoch: 1 step: 780, loss is 2.0695419311523438\n",
      "epoch: 1 step: 781, loss is 2.0531399250030518\n",
      "epoch: 1 step: 782, loss is 1.759907603263855\n",
      "epoch: 1 step: 783, loss is 1.8807933330535889\n",
      "epoch: 1 step: 784, loss is 1.8900389671325684\n",
      "epoch: 1 step: 785, loss is 2.1266977787017822\n",
      "epoch: 1 step: 786, loss is 2.012427806854248\n",
      "epoch: 1 step: 787, loss is 1.7738103866577148\n",
      "epoch: 1 step: 788, loss is 1.8420830965042114\n",
      "epoch: 1 step: 789, loss is 1.7606290578842163\n",
      "epoch: 1 step: 790, loss is 1.978367805480957\n",
      "epoch: 1 step: 791, loss is 1.6309961080551147\n",
      "epoch: 1 step: 792, loss is 2.060497999191284\n",
      "epoch: 1 step: 793, loss is 1.761085033416748\n",
      "epoch: 1 step: 794, loss is 1.9706217050552368\n",
      "epoch: 1 step: 795, loss is 1.678914189338684\n",
      "epoch: 1 step: 796, loss is 2.0617926120758057\n",
      "epoch: 1 step: 797, loss is 1.7717350721359253\n",
      "epoch: 1 step: 798, loss is 2.036433219909668\n",
      "epoch: 1 step: 799, loss is 1.5833951234817505\n",
      "epoch: 1 step: 800, loss is 1.7612613439559937\n",
      "epoch: 1 step: 801, loss is 1.783402681350708\n",
      "epoch: 1 step: 802, loss is 2.010981321334839\n",
      "epoch: 1 step: 803, loss is 1.7573871612548828\n",
      "epoch: 1 step: 804, loss is 2.0226213932037354\n",
      "epoch: 1 step: 805, loss is 1.9647785425186157\n",
      "epoch: 1 step: 806, loss is 1.9273074865341187\n",
      "epoch: 1 step: 807, loss is 1.8829978704452515\n",
      "epoch: 1 step: 808, loss is 1.9607980251312256\n",
      "epoch: 1 step: 809, loss is 1.8907122611999512\n",
      "epoch: 1 step: 810, loss is 1.6856474876403809\n",
      "epoch: 1 step: 811, loss is 1.903934121131897\n",
      "epoch: 1 step: 812, loss is 1.8297573328018188\n",
      "epoch: 1 step: 813, loss is 1.8394486904144287\n",
      "epoch: 1 step: 814, loss is 1.8025425672531128\n",
      "epoch: 1 step: 815, loss is 1.9051125049591064\n",
      "epoch: 1 step: 816, loss is 1.8245937824249268\n",
      "epoch: 1 step: 817, loss is 1.9495362043380737\n",
      "epoch: 1 step: 818, loss is 1.8553308248519897\n",
      "epoch: 1 step: 819, loss is 1.7115846872329712\n",
      "epoch: 1 step: 820, loss is 1.7649996280670166\n",
      "epoch: 1 step: 821, loss is 1.6904314756393433\n",
      "epoch: 1 step: 822, loss is 1.7325724363327026\n",
      "epoch: 1 step: 823, loss is 1.8402762413024902\n",
      "epoch: 1 step: 824, loss is 1.7554652690887451\n",
      "epoch: 1 step: 825, loss is 1.6633402109146118\n",
      "epoch: 1 step: 826, loss is 1.8072640895843506\n",
      "epoch: 1 step: 827, loss is 1.882633924484253\n",
      "epoch: 1 step: 828, loss is 1.6549382209777832\n",
      "epoch: 1 step: 829, loss is 1.6685824394226074\n",
      "epoch: 1 step: 830, loss is 1.8634947538375854\n",
      "epoch: 1 step: 831, loss is 1.877223253250122\n",
      "epoch: 1 step: 832, loss is 1.7932558059692383\n",
      "epoch: 1 step: 833, loss is 1.6177104711532593\n",
      "epoch: 1 step: 834, loss is 1.7407710552215576\n",
      "epoch: 1 step: 835, loss is 1.9477108716964722\n",
      "epoch: 1 step: 836, loss is 1.952772617340088\n",
      "epoch: 1 step: 837, loss is 1.8530951738357544\n",
      "epoch: 1 step: 838, loss is 1.7985422611236572\n",
      "epoch: 1 step: 839, loss is 2.06760835647583\n",
      "epoch: 1 step: 840, loss is 1.8516956567764282\n",
      "epoch: 1 step: 841, loss is 1.7309815883636475\n",
      "epoch: 1 step: 842, loss is 1.627131462097168\n",
      "epoch: 1 step: 843, loss is 1.7594537734985352\n",
      "epoch: 1 step: 844, loss is 1.4907511472702026\n",
      "epoch: 1 step: 845, loss is 1.944770097732544\n",
      "epoch: 1 step: 846, loss is 1.527458906173706\n",
      "epoch: 1 step: 847, loss is 2.0651042461395264\n",
      "epoch: 1 step: 848, loss is 1.931606650352478\n",
      "epoch: 1 step: 849, loss is 1.9211671352386475\n",
      "epoch: 1 step: 850, loss is 2.0037801265716553\n",
      "epoch: 1 step: 851, loss is 1.9429380893707275\n",
      "epoch: 1 step: 852, loss is 1.8902844190597534\n",
      "epoch: 1 step: 853, loss is 1.6058015823364258\n",
      "epoch: 1 step: 854, loss is 1.5089654922485352\n",
      "epoch: 1 step: 855, loss is 1.7654637098312378\n",
      "epoch: 1 step: 856, loss is 1.8068687915802002\n",
      "epoch: 1 step: 857, loss is 1.6243667602539062\n",
      "epoch: 1 step: 858, loss is 1.8479832410812378\n",
      "epoch: 1 step: 859, loss is 1.7081836462020874\n",
      "epoch: 1 step: 860, loss is 1.6670055389404297\n",
      "epoch: 1 step: 861, loss is 1.7287689447402954\n",
      "epoch: 1 step: 862, loss is 1.8653335571289062\n",
      "epoch: 1 step: 863, loss is 2.1066811084747314\n",
      "epoch: 1 step: 864, loss is 1.9668946266174316\n",
      "epoch: 1 step: 865, loss is 1.907644271850586\n",
      "epoch: 1 step: 866, loss is 1.7246742248535156\n",
      "epoch: 1 step: 867, loss is 1.9004840850830078\n",
      "epoch: 1 step: 868, loss is 1.7988938093185425\n",
      "epoch: 1 step: 869, loss is 1.5198231935501099\n",
      "epoch: 1 step: 870, loss is 1.9088338613510132\n",
      "epoch: 1 step: 871, loss is 1.5258556604385376\n",
      "epoch: 1 step: 872, loss is 1.6345988512039185\n",
      "epoch: 1 step: 873, loss is 1.8613520860671997\n",
      "epoch: 1 step: 874, loss is 1.6809885501861572\n",
      "epoch: 1 step: 875, loss is 1.7615278959274292\n",
      "epoch: 1 step: 876, loss is 2.0400614738464355\n",
      "epoch: 1 step: 877, loss is 1.699148178100586\n",
      "epoch: 1 step: 878, loss is 1.666036605834961\n",
      "epoch: 1 step: 879, loss is 1.7413713932037354\n",
      "epoch: 1 step: 880, loss is 1.6305499076843262\n",
      "epoch: 1 step: 881, loss is 1.7175848484039307\n",
      "epoch: 1 step: 882, loss is 1.875697374343872\n",
      "epoch: 1 step: 883, loss is 1.7048354148864746\n",
      "epoch: 1 step: 884, loss is 1.78734290599823\n",
      "epoch: 1 step: 885, loss is 1.7685811519622803\n",
      "epoch: 1 step: 886, loss is 1.8910186290740967\n",
      "epoch: 1 step: 887, loss is 1.7553154230117798\n",
      "epoch: 1 step: 888, loss is 1.851306438446045\n",
      "epoch: 1 step: 889, loss is 1.8865362405776978\n",
      "epoch: 1 step: 890, loss is 1.9682645797729492\n",
      "epoch: 1 step: 891, loss is 1.9011629819869995\n",
      "epoch: 1 step: 892, loss is 1.6330409049987793\n",
      "epoch: 1 step: 893, loss is 1.5907258987426758\n",
      "epoch: 1 step: 894, loss is 1.7284176349639893\n",
      "epoch: 1 step: 895, loss is 1.8013873100280762\n",
      "epoch: 1 step: 896, loss is 1.6828020811080933\n",
      "epoch: 1 step: 897, loss is 1.7226836681365967\n",
      "epoch: 1 step: 898, loss is 1.6282256841659546\n",
      "epoch: 1 step: 899, loss is 1.851986050605774\n",
      "epoch: 1 step: 900, loss is 1.949829339981079\n",
      "epoch: 1 step: 901, loss is 1.8094799518585205\n",
      "epoch: 1 step: 902, loss is 2.050302743911743\n",
      "epoch: 1 step: 903, loss is 1.6472727060317993\n",
      "epoch: 1 step: 904, loss is 1.804206132888794\n",
      "epoch: 1 step: 905, loss is 1.6294082403182983\n",
      "epoch: 1 step: 906, loss is 1.8396499156951904\n",
      "epoch: 1 step: 907, loss is 1.6809191703796387\n",
      "epoch: 1 step: 908, loss is 1.7254183292388916\n",
      "epoch: 1 step: 909, loss is 1.6434820890426636\n",
      "epoch: 1 step: 910, loss is 1.7271404266357422\n",
      "epoch: 1 step: 911, loss is 1.9393633604049683\n",
      "epoch: 1 step: 912, loss is 1.7866655588150024\n",
      "epoch: 1 step: 913, loss is 1.6253938674926758\n",
      "epoch: 1 step: 914, loss is 1.569556474685669\n",
      "epoch: 1 step: 915, loss is 1.7455912828445435\n",
      "epoch: 1 step: 916, loss is 1.6749604940414429\n",
      "epoch: 1 step: 917, loss is 1.9169714450836182\n",
      "epoch: 1 step: 918, loss is 1.6120936870574951\n",
      "epoch: 1 step: 919, loss is 1.8060482740402222\n",
      "epoch: 1 step: 920, loss is 1.514342188835144\n",
      "epoch: 1 step: 921, loss is 1.6898216009140015\n",
      "epoch: 1 step: 922, loss is 1.752573013305664\n",
      "epoch: 1 step: 923, loss is 1.8395214080810547\n",
      "epoch: 1 step: 924, loss is 1.9876996278762817\n",
      "epoch: 1 step: 925, loss is 1.9044113159179688\n",
      "epoch: 1 step: 926, loss is 1.7805674076080322\n",
      "epoch: 1 step: 927, loss is 1.6436030864715576\n",
      "epoch: 1 step: 928, loss is 1.6479014158248901\n",
      "epoch: 1 step: 929, loss is 1.6558359861373901\n",
      "epoch: 1 step: 930, loss is 1.5451016426086426\n",
      "epoch: 1 step: 931, loss is 1.7359035015106201\n",
      "epoch: 1 step: 932, loss is 1.4347114562988281\n",
      "epoch: 1 step: 933, loss is 1.4113143682479858\n",
      "epoch: 1 step: 934, loss is 1.7471247911453247\n",
      "epoch: 1 step: 935, loss is 1.6005679368972778\n",
      "epoch: 1 step: 936, loss is 1.6663501262664795\n",
      "epoch: 1 step: 937, loss is 1.588318943977356\n",
      "epoch: 1 step: 938, loss is 1.5485117435455322\n",
      "epoch: 1 step: 939, loss is 1.992911458015442\n",
      "epoch: 1 step: 940, loss is 1.6400697231292725\n",
      "epoch: 1 step: 941, loss is 1.851886510848999\n",
      "epoch: 1 step: 942, loss is 1.5412641763687134\n",
      "epoch: 1 step: 943, loss is 1.7318894863128662\n",
      "epoch: 1 step: 944, loss is 1.8115931749343872\n",
      "epoch: 1 step: 945, loss is 2.0266218185424805\n",
      "epoch: 1 step: 946, loss is 1.9142693281173706\n",
      "epoch: 1 step: 947, loss is 1.814225435256958\n",
      "epoch: 1 step: 948, loss is 1.5114102363586426\n",
      "epoch: 1 step: 949, loss is 1.5433623790740967\n",
      "epoch: 1 step: 950, loss is 1.827696442604065\n",
      "epoch: 1 step: 951, loss is 1.5522514581680298\n",
      "epoch: 1 step: 952, loss is 1.7068678140640259\n",
      "epoch: 1 step: 953, loss is 1.89614999294281\n",
      "epoch: 1 step: 954, loss is 1.9199820756912231\n",
      "epoch: 1 step: 955, loss is 1.8515949249267578\n",
      "epoch: 1 step: 956, loss is 1.4594827890396118\n",
      "epoch: 1 step: 957, loss is 1.8670332431793213\n",
      "epoch: 1 step: 958, loss is 1.5266646146774292\n",
      "epoch: 1 step: 959, loss is 1.9135092496871948\n",
      "epoch: 1 step: 960, loss is 2.2453572750091553\n",
      "epoch: 1 step: 961, loss is 1.8200851678848267\n",
      "epoch: 1 step: 962, loss is 1.731683373451233\n",
      "epoch: 1 step: 963, loss is 1.744502067565918\n",
      "epoch: 1 step: 964, loss is 1.4979352951049805\n",
      "epoch: 1 step: 965, loss is 1.542905569076538\n",
      "epoch: 1 step: 966, loss is 2.148972272872925\n",
      "epoch: 1 step: 967, loss is 1.97559654712677\n",
      "epoch: 1 step: 968, loss is 1.7725733518600464\n",
      "epoch: 1 step: 969, loss is 1.6463345289230347\n",
      "epoch: 1 step: 970, loss is 1.9201775789260864\n",
      "epoch: 1 step: 971, loss is 1.5146828889846802\n",
      "epoch: 1 step: 972, loss is 1.5725058317184448\n",
      "epoch: 1 step: 973, loss is 1.657353401184082\n",
      "epoch: 1 step: 974, loss is 1.8188546895980835\n",
      "epoch: 1 step: 975, loss is 1.6304295063018799\n",
      "epoch: 1 step: 976, loss is 1.8004332780838013\n",
      "epoch: 1 step: 977, loss is 1.736682653427124\n",
      "epoch: 1 step: 978, loss is 1.751624584197998\n",
      "epoch: 1 step: 979, loss is 1.8643091917037964\n",
      "epoch: 1 step: 980, loss is 1.7654354572296143\n",
      "epoch: 1 step: 981, loss is 1.43906831741333\n",
      "epoch: 1 step: 982, loss is 1.777151107788086\n",
      "epoch: 1 step: 983, loss is 1.6486515998840332\n",
      "epoch: 1 step: 984, loss is 1.947487473487854\n",
      "epoch: 1 step: 985, loss is 1.9010345935821533\n",
      "epoch: 1 step: 986, loss is 1.5017708539962769\n",
      "epoch: 1 step: 987, loss is 1.7046034336090088\n",
      "epoch: 1 step: 988, loss is 1.8023698329925537\n",
      "epoch: 1 step: 989, loss is 1.7064213752746582\n",
      "epoch: 1 step: 990, loss is 2.015057325363159\n",
      "epoch: 1 step: 991, loss is 1.8297200202941895\n",
      "epoch: 1 step: 992, loss is 1.7647782564163208\n",
      "epoch: 1 step: 993, loss is 1.784117579460144\n",
      "epoch: 1 step: 994, loss is 1.6776477098464966\n",
      "epoch: 1 step: 995, loss is 1.9174884557724\n",
      "epoch: 1 step: 996, loss is 1.855560064315796\n",
      "epoch: 1 step: 997, loss is 1.5578906536102295\n",
      "epoch: 1 step: 998, loss is 1.4455208778381348\n",
      "epoch: 1 step: 999, loss is 1.6405181884765625\n",
      "epoch: 1 step: 1000, loss is 1.5617910623550415\n",
      "epoch: 1 step: 1001, loss is 1.6961402893066406\n",
      "epoch: 1 step: 1002, loss is 1.9539084434509277\n",
      "epoch: 1 step: 1003, loss is 1.7481577396392822\n",
      "epoch: 1 step: 1004, loss is 1.7782355546951294\n",
      "epoch: 1 step: 1005, loss is 1.79140043258667\n",
      "epoch: 1 step: 1006, loss is 1.752990484237671\n",
      "epoch: 1 step: 1007, loss is 1.7137337923049927\n",
      "epoch: 1 step: 1008, loss is 1.6650179624557495\n",
      "epoch: 1 step: 1009, loss is 1.5960493087768555\n",
      "epoch: 1 step: 1010, loss is 1.5206129550933838\n",
      "epoch: 1 step: 1011, loss is 2.1203441619873047\n",
      "epoch: 1 step: 1012, loss is 1.6515257358551025\n",
      "epoch: 1 step: 1013, loss is 1.7024589776992798\n",
      "epoch: 1 step: 1014, loss is 1.8527929782867432\n",
      "epoch: 1 step: 1015, loss is 1.3731356859207153\n",
      "epoch: 1 step: 1016, loss is 1.477390170097351\n",
      "epoch: 1 step: 1017, loss is 1.983026385307312\n",
      "epoch: 1 step: 1018, loss is 2.263014078140259\n",
      "epoch: 1 step: 1019, loss is 1.90623939037323\n",
      "epoch: 1 step: 1020, loss is 1.6959232091903687\n",
      "epoch: 1 step: 1021, loss is 1.5984872579574585\n",
      "epoch: 1 step: 1022, loss is 1.6765763759613037\n",
      "epoch: 1 step: 1023, loss is 1.9468762874603271\n",
      "epoch: 1 step: 1024, loss is 1.6223949193954468\n",
      "epoch: 1 step: 1025, loss is 2.0090839862823486\n",
      "epoch: 1 step: 1026, loss is 1.9791604280471802\n",
      "epoch: 1 step: 1027, loss is 1.7144757509231567\n",
      "epoch: 1 step: 1028, loss is 1.5302088260650635\n",
      "epoch: 1 step: 1029, loss is 1.5621258020401\n",
      "epoch: 1 step: 1030, loss is 1.6474772691726685\n",
      "epoch: 1 step: 1031, loss is 1.8674814701080322\n",
      "epoch: 1 step: 1032, loss is 1.6754815578460693\n",
      "epoch: 1 step: 1033, loss is 1.6197400093078613\n",
      "epoch: 1 step: 1034, loss is 1.8494645357131958\n",
      "epoch: 1 step: 1035, loss is 1.666008472442627\n",
      "epoch: 1 step: 1036, loss is 1.711051106452942\n",
      "epoch: 1 step: 1037, loss is 1.4713172912597656\n",
      "epoch: 1 step: 1038, loss is 1.6060856580734253\n",
      "epoch: 1 step: 1039, loss is 1.59451162815094\n",
      "epoch: 1 step: 1040, loss is 1.5152250528335571\n",
      "epoch: 1 step: 1041, loss is 1.7453447580337524\n",
      "epoch: 1 step: 1042, loss is 1.3813376426696777\n",
      "epoch: 1 step: 1043, loss is 1.5512504577636719\n",
      "epoch: 1 step: 1044, loss is 1.950600266456604\n",
      "epoch: 1 step: 1045, loss is 1.6209102869033813\n",
      "epoch: 1 step: 1046, loss is 1.7192113399505615\n",
      "epoch: 1 step: 1047, loss is 1.7884314060211182\n",
      "epoch: 1 step: 1048, loss is 1.6417005062103271\n",
      "epoch: 1 step: 1049, loss is 1.768214464187622\n",
      "epoch: 1 step: 1050, loss is 1.84964120388031\n",
      "epoch: 1 step: 1051, loss is 1.6359622478485107\n",
      "epoch: 1 step: 1052, loss is 1.7924747467041016\n",
      "epoch: 1 step: 1053, loss is 1.8595705032348633\n",
      "epoch: 1 step: 1054, loss is 1.8403916358947754\n",
      "epoch: 1 step: 1055, loss is 1.7448735237121582\n",
      "epoch: 1 step: 1056, loss is 1.5074036121368408\n",
      "epoch: 1 step: 1057, loss is 1.6067042350769043\n",
      "epoch: 1 step: 1058, loss is 1.6071923971176147\n",
      "epoch: 1 step: 1059, loss is 2.0647895336151123\n",
      "epoch: 1 step: 1060, loss is 1.639316439628601\n",
      "epoch: 1 step: 1061, loss is 1.7228987216949463\n",
      "epoch: 1 step: 1062, loss is 1.7002341747283936\n",
      "epoch: 1 step: 1063, loss is 1.8130509853363037\n",
      "epoch: 1 step: 1064, loss is 1.6807632446289062\n",
      "epoch: 1 step: 1065, loss is 1.6574019193649292\n",
      "epoch: 1 step: 1066, loss is 1.6633096933364868\n",
      "epoch: 1 step: 1067, loss is 1.6031415462493896\n",
      "epoch: 1 step: 1068, loss is 1.9524041414260864\n",
      "epoch: 1 step: 1069, loss is 1.8099424839019775\n",
      "epoch: 1 step: 1070, loss is 1.5375957489013672\n",
      "epoch: 1 step: 1071, loss is 1.6488412618637085\n",
      "epoch: 1 step: 1072, loss is 1.6391457319259644\n",
      "epoch: 1 step: 1073, loss is 1.7553184032440186\n",
      "epoch: 1 step: 1074, loss is 1.5683714151382446\n",
      "epoch: 1 step: 1075, loss is 1.8035515546798706\n",
      "epoch: 1 step: 1076, loss is 1.5897139310836792\n",
      "epoch: 1 step: 1077, loss is 1.3298919200897217\n",
      "epoch: 1 step: 1078, loss is 1.549189567565918\n",
      "epoch: 1 step: 1079, loss is 1.5114907026290894\n",
      "epoch: 1 step: 1080, loss is 1.6543375253677368\n",
      "epoch: 1 step: 1081, loss is 1.5727434158325195\n",
      "epoch: 1 step: 1082, loss is 1.5035626888275146\n",
      "epoch: 1 step: 1083, loss is 1.5858514308929443\n",
      "epoch: 1 step: 1084, loss is 1.7800333499908447\n",
      "epoch: 1 step: 1085, loss is 1.7269682884216309\n",
      "epoch: 1 step: 1086, loss is 1.5981833934783936\n",
      "epoch: 1 step: 1087, loss is 1.6072837114334106\n",
      "epoch: 1 step: 1088, loss is 2.025094509124756\n",
      "epoch: 1 step: 1089, loss is 1.641114354133606\n",
      "epoch: 1 step: 1090, loss is 1.793182611465454\n",
      "epoch: 1 step: 1091, loss is 1.573320746421814\n",
      "epoch: 1 step: 1092, loss is 1.6642566919326782\n",
      "epoch: 1 step: 1093, loss is 1.562178134918213\n",
      "epoch: 1 step: 1094, loss is 1.6050893068313599\n",
      "epoch: 1 step: 1095, loss is 1.75080144405365\n",
      "epoch: 1 step: 1096, loss is 1.778924584388733\n",
      "epoch: 1 step: 1097, loss is 1.4141827821731567\n",
      "epoch: 1 step: 1098, loss is 1.4750831127166748\n",
      "epoch: 1 step: 1099, loss is 1.7601649761199951\n",
      "epoch: 1 step: 1100, loss is 1.7230393886566162\n",
      "epoch: 1 step: 1101, loss is 1.653416395187378\n",
      "epoch: 1 step: 1102, loss is 1.669604778289795\n",
      "epoch: 1 step: 1103, loss is 1.5667234659194946\n",
      "epoch: 1 step: 1104, loss is 1.7123774290084839\n",
      "epoch: 1 step: 1105, loss is 1.8348079919815063\n",
      "epoch: 1 step: 1106, loss is 1.6464056968688965\n",
      "epoch: 1 step: 1107, loss is 1.7030694484710693\n",
      "epoch: 1 step: 1108, loss is 1.5699565410614014\n",
      "epoch: 1 step: 1109, loss is 1.9038887023925781\n",
      "epoch: 1 step: 1110, loss is 1.6389386653900146\n",
      "epoch: 1 step: 1111, loss is 1.7348716259002686\n",
      "epoch: 1 step: 1112, loss is 2.0908870697021484\n",
      "epoch: 1 step: 1113, loss is 1.7428439855575562\n",
      "epoch: 1 step: 1114, loss is 1.7985507249832153\n",
      "epoch: 1 step: 1115, loss is 1.6571578979492188\n",
      "epoch: 1 step: 1116, loss is 1.4140740633010864\n",
      "epoch: 1 step: 1117, loss is 1.403896450996399\n",
      "epoch: 1 step: 1118, loss is 1.9495747089385986\n",
      "epoch: 1 step: 1119, loss is 1.671026349067688\n",
      "epoch: 1 step: 1120, loss is 1.645716905593872\n",
      "epoch: 1 step: 1121, loss is 1.3348991870880127\n",
      "epoch: 1 step: 1122, loss is 1.8086661100387573\n",
      "epoch: 1 step: 1123, loss is 1.486388087272644\n",
      "epoch: 1 step: 1124, loss is 1.7615774869918823\n",
      "epoch: 1 step: 1125, loss is 1.8751291036605835\n",
      "epoch: 1 step: 1126, loss is 1.5619571208953857\n",
      "epoch: 1 step: 1127, loss is 1.6043033599853516\n",
      "epoch: 1 step: 1128, loss is 1.689107894897461\n",
      "epoch: 1 step: 1129, loss is 1.9428969621658325\n",
      "epoch: 1 step: 1130, loss is 1.6208783388137817\n",
      "epoch: 1 step: 1131, loss is 1.4144060611724854\n",
      "epoch: 1 step: 1132, loss is 1.5285342931747437\n",
      "epoch: 1 step: 1133, loss is 1.8234717845916748\n",
      "epoch: 1 step: 1134, loss is 1.7360135316848755\n",
      "epoch: 1 step: 1135, loss is 1.5883979797363281\n",
      "epoch: 1 step: 1136, loss is 1.654768705368042\n",
      "epoch: 1 step: 1137, loss is 1.5922504663467407\n",
      "epoch: 1 step: 1138, loss is 1.5790696144104004\n",
      "epoch: 1 step: 1139, loss is 1.5173250436782837\n",
      "epoch: 1 step: 1140, loss is 1.5815260410308838\n",
      "epoch: 1 step: 1141, loss is 1.9165312051773071\n",
      "epoch: 1 step: 1142, loss is 2.0311596393585205\n",
      "epoch: 1 step: 1143, loss is 1.7074288129806519\n",
      "epoch: 1 step: 1144, loss is 1.5789347887039185\n",
      "epoch: 1 step: 1145, loss is 1.6557464599609375\n",
      "epoch: 1 step: 1146, loss is 1.417572259902954\n",
      "epoch: 1 step: 1147, loss is 1.5956817865371704\n",
      "epoch: 1 step: 1148, loss is 1.801413655281067\n",
      "epoch: 1 step: 1149, loss is 1.612711787223816\n",
      "epoch: 1 step: 1150, loss is 1.576454520225525\n",
      "epoch: 1 step: 1151, loss is 1.8505498170852661\n",
      "epoch: 1 step: 1152, loss is 1.6256159543991089\n",
      "epoch: 1 step: 1153, loss is 1.692970633506775\n",
      "epoch: 1 step: 1154, loss is 1.7784755229949951\n",
      "epoch: 1 step: 1155, loss is 1.84959077835083\n",
      "epoch: 1 step: 1156, loss is 1.9084936380386353\n",
      "epoch: 1 step: 1157, loss is 1.8987172842025757\n",
      "epoch: 1 step: 1158, loss is 1.5441019535064697\n",
      "epoch: 1 step: 1159, loss is 1.7760409116744995\n",
      "epoch: 1 step: 1160, loss is 1.8785338401794434\n",
      "epoch: 1 step: 1161, loss is 1.5153260231018066\n",
      "epoch: 1 step: 1162, loss is 1.5503748655319214\n",
      "epoch: 1 step: 1163, loss is 2.106722593307495\n",
      "epoch: 1 step: 1164, loss is 1.614281415939331\n",
      "epoch: 1 step: 1165, loss is 1.8045567274093628\n",
      "epoch: 1 step: 1166, loss is 1.7051782608032227\n",
      "epoch: 1 step: 1167, loss is 1.681804895401001\n",
      "epoch: 1 step: 1168, loss is 1.5574296712875366\n",
      "epoch: 1 step: 1169, loss is 1.8018717765808105\n",
      "epoch: 1 step: 1170, loss is 1.6967830657958984\n",
      "epoch: 1 step: 1171, loss is 1.7506656646728516\n",
      "epoch: 1 step: 1172, loss is 1.6946250200271606\n",
      "epoch: 1 step: 1173, loss is 1.8389307260513306\n",
      "epoch: 1 step: 1174, loss is 1.655224323272705\n",
      "epoch: 1 step: 1175, loss is 1.829352855682373\n",
      "epoch: 1 step: 1176, loss is 1.52251398563385\n",
      "epoch: 1 step: 1177, loss is 1.5626842975616455\n",
      "epoch: 1 step: 1178, loss is 1.9187260866165161\n",
      "epoch: 1 step: 1179, loss is 1.7682583332061768\n",
      "epoch: 1 step: 1180, loss is 1.622436285018921\n",
      "epoch: 1 step: 1181, loss is 1.6083002090454102\n",
      "epoch: 1 step: 1182, loss is 1.5848498344421387\n",
      "epoch: 1 step: 1183, loss is 1.3164738416671753\n",
      "epoch: 1 step: 1184, loss is 1.669040322303772\n",
      "epoch: 1 step: 1185, loss is 1.487119197845459\n",
      "epoch: 1 step: 1186, loss is 1.9153188467025757\n",
      "epoch: 1 step: 1187, loss is 1.5939114093780518\n",
      "epoch: 1 step: 1188, loss is 1.5556449890136719\n",
      "epoch: 1 step: 1189, loss is 1.6777114868164062\n",
      "epoch: 1 step: 1190, loss is 1.7627853155136108\n",
      "epoch: 1 step: 1191, loss is 1.5477770566940308\n",
      "epoch: 1 step: 1192, loss is 1.658349633216858\n",
      "epoch: 1 step: 1193, loss is 1.461377501487732\n",
      "epoch: 1 step: 1194, loss is 1.9937421083450317\n",
      "epoch: 1 step: 1195, loss is 1.7270394563674927\n",
      "epoch: 1 step: 1196, loss is 1.579553246498108\n",
      "epoch: 1 step: 1197, loss is 1.6153485774993896\n",
      "epoch: 1 step: 1198, loss is 1.791232943534851\n",
      "epoch: 1 step: 1199, loss is 1.9919580221176147\n",
      "epoch: 1 step: 1200, loss is 1.6491923332214355\n",
      "epoch: 1 step: 1201, loss is 1.8243552446365356\n",
      "epoch: 1 step: 1202, loss is 1.6667306423187256\n",
      "epoch: 1 step: 1203, loss is 1.5792750120162964\n",
      "epoch: 1 step: 1204, loss is 1.8576985597610474\n",
      "epoch: 1 step: 1205, loss is 1.595003366470337\n",
      "epoch: 1 step: 1206, loss is 1.7123568058013916\n",
      "epoch: 1 step: 1207, loss is 1.4366556406021118\n",
      "epoch: 1 step: 1208, loss is 1.538020372390747\n",
      "epoch: 1 step: 1209, loss is 1.774063229560852\n",
      "epoch: 1 step: 1210, loss is 1.5674959421157837\n",
      "epoch: 1 step: 1211, loss is 1.5063084363937378\n",
      "epoch: 1 step: 1212, loss is 1.575715184211731\n",
      "epoch: 1 step: 1213, loss is 1.670708417892456\n",
      "epoch: 1 step: 1214, loss is 1.4297178983688354\n",
      "epoch: 1 step: 1215, loss is 1.4825519323349\n",
      "epoch: 1 step: 1216, loss is 1.4745663404464722\n",
      "epoch: 1 step: 1217, loss is 1.418407917022705\n",
      "epoch: 1 step: 1218, loss is 1.5844510793685913\n",
      "epoch: 1 step: 1219, loss is 1.8322869539260864\n",
      "epoch: 1 step: 1220, loss is 1.396639347076416\n",
      "epoch: 1 step: 1221, loss is 1.7534197568893433\n",
      "epoch: 1 step: 1222, loss is 1.8124597072601318\n",
      "epoch: 1 step: 1223, loss is 1.6325056552886963\n",
      "epoch: 1 step: 1224, loss is 1.466475248336792\n",
      "epoch: 1 step: 1225, loss is 1.695074439048767\n",
      "epoch: 1 step: 1226, loss is 1.6256561279296875\n",
      "epoch: 1 step: 1227, loss is 1.8920793533325195\n",
      "epoch: 1 step: 1228, loss is 1.8129044771194458\n",
      "epoch: 1 step: 1229, loss is 1.6127216815948486\n",
      "epoch: 1 step: 1230, loss is 1.418196439743042\n",
      "epoch: 1 step: 1231, loss is 1.4946473836898804\n",
      "epoch: 1 step: 1232, loss is 1.7518104314804077\n",
      "epoch: 1 step: 1233, loss is 1.6655614376068115\n",
      "epoch: 1 step: 1234, loss is 1.9660648107528687\n",
      "epoch: 1 step: 1235, loss is 1.6504971981048584\n",
      "epoch: 1 step: 1236, loss is 1.524900197982788\n",
      "epoch: 1 step: 1237, loss is 1.5948423147201538\n",
      "epoch: 1 step: 1238, loss is 1.6954025030136108\n",
      "epoch: 1 step: 1239, loss is 1.5631474256515503\n",
      "epoch: 1 step: 1240, loss is 1.56208074092865\n",
      "epoch: 1 step: 1241, loss is 1.775115966796875\n",
      "epoch: 1 step: 1242, loss is 1.566635251045227\n",
      "epoch: 1 step: 1243, loss is 1.8664805889129639\n",
      "epoch: 1 step: 1244, loss is 1.7224352359771729\n",
      "epoch: 1 step: 1245, loss is 1.4691226482391357\n",
      "epoch: 1 step: 1246, loss is 1.5128967761993408\n",
      "epoch: 1 step: 1247, loss is 1.4166433811187744\n",
      "epoch: 1 step: 1248, loss is 1.4038503170013428\n",
      "epoch: 1 step: 1249, loss is 1.5298932790756226\n",
      "epoch: 1 step: 1250, loss is 1.7376761436462402\n",
      "epoch: 1 step: 1251, loss is 1.2920256853103638\n",
      "epoch: 1 step: 1252, loss is 1.4342234134674072\n",
      "epoch: 1 step: 1253, loss is 1.7533996105194092\n",
      "epoch: 1 step: 1254, loss is 1.3560665845870972\n",
      "epoch: 1 step: 1255, loss is 1.7473512887954712\n",
      "epoch: 1 step: 1256, loss is 1.5933196544647217\n",
      "epoch: 1 step: 1257, loss is 1.6955821514129639\n",
      "epoch: 1 step: 1258, loss is 1.257863163948059\n",
      "epoch: 1 step: 1259, loss is 1.5815399885177612\n",
      "epoch: 1 step: 1260, loss is 1.3288416862487793\n",
      "epoch: 1 step: 1261, loss is 1.7484259605407715\n",
      "epoch: 1 step: 1262, loss is 1.3872411251068115\n",
      "epoch: 1 step: 1263, loss is 1.2825562953948975\n",
      "epoch: 1 step: 1264, loss is 1.923822045326233\n",
      "epoch: 1 step: 1265, loss is 1.4823614358901978\n",
      "epoch: 1 step: 1266, loss is 1.705857515335083\n",
      "epoch: 1 step: 1267, loss is 1.6053502559661865\n",
      "epoch: 1 step: 1268, loss is 1.2495144605636597\n",
      "epoch: 1 step: 1269, loss is 1.7827403545379639\n",
      "epoch: 1 step: 1270, loss is 1.5931476354599\n",
      "epoch: 1 step: 1271, loss is 1.3461183309555054\n",
      "epoch: 1 step: 1272, loss is 1.396345853805542\n",
      "epoch: 1 step: 1273, loss is 2.4081408977508545\n",
      "epoch: 1 step: 1274, loss is 1.4074666500091553\n",
      "epoch: 1 step: 1275, loss is 1.434471607208252\n",
      "epoch: 1 step: 1276, loss is 1.502176284790039\n",
      "epoch: 1 step: 1277, loss is 1.386693000793457\n",
      "epoch: 1 step: 1278, loss is 1.6140549182891846\n",
      "epoch: 1 step: 1279, loss is 1.6816492080688477\n",
      "epoch: 1 step: 1280, loss is 1.271134614944458\n",
      "epoch: 1 step: 1281, loss is 1.4660165309906006\n",
      "epoch: 1 step: 1282, loss is 1.5775662660598755\n",
      "epoch: 1 step: 1283, loss is 1.6826598644256592\n",
      "epoch: 1 step: 1284, loss is 1.603127121925354\n",
      "epoch: 1 step: 1285, loss is 1.7866976261138916\n",
      "epoch: 1 step: 1286, loss is 1.637190818786621\n",
      "epoch: 1 step: 1287, loss is 1.6182600259780884\n",
      "epoch: 1 step: 1288, loss is 1.383266806602478\n",
      "epoch: 1 step: 1289, loss is 1.666261076927185\n",
      "epoch: 1 step: 1290, loss is 1.615188479423523\n",
      "epoch: 1 step: 1291, loss is 1.4425150156021118\n",
      "epoch: 1 step: 1292, loss is 1.5232239961624146\n",
      "epoch: 1 step: 1293, loss is 1.696617841720581\n",
      "epoch: 1 step: 1294, loss is 1.7945493459701538\n",
      "epoch: 1 step: 1295, loss is 1.6007909774780273\n",
      "epoch: 1 step: 1296, loss is 1.6390436887741089\n",
      "epoch: 1 step: 1297, loss is 1.6762129068374634\n",
      "epoch: 1 step: 1298, loss is 1.614882230758667\n",
      "epoch: 1 step: 1299, loss is 1.721224308013916\n",
      "epoch: 1 step: 1300, loss is 1.63368821144104\n",
      "epoch: 1 step: 1301, loss is 1.440564513206482\n",
      "epoch: 1 step: 1302, loss is 1.5242445468902588\n",
      "epoch: 1 step: 1303, loss is 1.291218876838684\n",
      "epoch: 1 step: 1304, loss is 1.6812620162963867\n",
      "epoch: 1 step: 1305, loss is 1.3753349781036377\n",
      "epoch: 1 step: 1306, loss is 1.9626790285110474\n",
      "epoch: 1 step: 1307, loss is 1.6914129257202148\n",
      "epoch: 1 step: 1308, loss is 1.4959009885787964\n",
      "epoch: 1 step: 1309, loss is 1.2703466415405273\n",
      "epoch: 1 step: 1310, loss is 1.9566824436187744\n",
      "epoch: 1 step: 1311, loss is 1.5549063682556152\n",
      "epoch: 1 step: 1312, loss is 1.5847852230072021\n",
      "epoch: 1 step: 1313, loss is 1.6043897867202759\n",
      "epoch: 1 step: 1314, loss is 1.628095269203186\n",
      "epoch: 1 step: 1315, loss is 2.0487141609191895\n",
      "epoch: 1 step: 1316, loss is 1.6101871728897095\n",
      "epoch: 1 step: 1317, loss is 1.6763447523117065\n",
      "epoch: 1 step: 1318, loss is 1.5081411600112915\n",
      "epoch: 1 step: 1319, loss is 1.6122560501098633\n",
      "epoch: 1 step: 1320, loss is 1.6353065967559814\n",
      "epoch: 1 step: 1321, loss is 1.8223782777786255\n",
      "epoch: 1 step: 1322, loss is 1.7918035984039307\n",
      "epoch: 1 step: 1323, loss is 1.7347743511199951\n",
      "epoch: 1 step: 1324, loss is 1.3534795045852661\n",
      "epoch: 1 step: 1325, loss is 1.4372307062149048\n",
      "epoch: 1 step: 1326, loss is 1.9025664329528809\n",
      "epoch: 1 step: 1327, loss is 1.874619722366333\n",
      "epoch: 1 step: 1328, loss is 1.7061258554458618\n",
      "epoch: 1 step: 1329, loss is 1.2623978853225708\n",
      "epoch: 1 step: 1330, loss is 1.5958970785140991\n",
      "epoch: 1 step: 1331, loss is 1.6560816764831543\n",
      "epoch: 1 step: 1332, loss is 1.6213631629943848\n",
      "epoch: 1 step: 1333, loss is 1.3923877477645874\n",
      "epoch: 1 step: 1334, loss is 1.7329623699188232\n",
      "epoch: 1 step: 1335, loss is 1.7645056247711182\n",
      "epoch: 1 step: 1336, loss is 1.5248746871948242\n",
      "epoch: 1 step: 1337, loss is 1.4508328437805176\n",
      "epoch: 1 step: 1338, loss is 1.4418785572052002\n",
      "epoch: 1 step: 1339, loss is 1.4369652271270752\n",
      "epoch: 1 step: 1340, loss is 1.7210153341293335\n",
      "epoch: 1 step: 1341, loss is 1.5151093006134033\n",
      "epoch: 1 step: 1342, loss is 1.5647978782653809\n",
      "epoch: 1 step: 1343, loss is 1.6713871955871582\n",
      "epoch: 1 step: 1344, loss is 1.5684266090393066\n",
      "epoch: 1 step: 1345, loss is 1.6086806058883667\n",
      "epoch: 1 step: 1346, loss is 1.4817676544189453\n",
      "epoch: 1 step: 1347, loss is 1.5155695676803589\n",
      "epoch: 1 step: 1348, loss is 1.5739690065383911\n",
      "epoch: 1 step: 1349, loss is 1.3993662595748901\n",
      "epoch: 1 step: 1350, loss is 1.3627028465270996\n",
      "epoch: 1 step: 1351, loss is 1.6168426275253296\n",
      "epoch: 1 step: 1352, loss is 1.5724337100982666\n",
      "epoch: 1 step: 1353, loss is 1.2341963052749634\n",
      "epoch: 1 step: 1354, loss is 1.4373403787612915\n",
      "epoch: 1 step: 1355, loss is 1.4295449256896973\n",
      "epoch: 1 step: 1356, loss is 1.518084168434143\n",
      "epoch: 1 step: 1357, loss is 1.5129998922348022\n",
      "epoch: 1 step: 1358, loss is 1.30324387550354\n",
      "epoch: 1 step: 1359, loss is 1.5400325059890747\n",
      "epoch: 1 step: 1360, loss is 1.5569493770599365\n",
      "epoch: 1 step: 1361, loss is 1.6619973182678223\n",
      "epoch: 1 step: 1362, loss is 1.6163887977600098\n",
      "epoch: 1 step: 1363, loss is 1.6686590909957886\n",
      "epoch: 1 step: 1364, loss is 1.6207269430160522\n",
      "epoch: 1 step: 1365, loss is 1.4564660787582397\n",
      "epoch: 1 step: 1366, loss is 1.6408822536468506\n",
      "epoch: 1 step: 1367, loss is 1.5081826448440552\n",
      "epoch: 1 step: 1368, loss is 1.5767053365707397\n",
      "epoch: 1 step: 1369, loss is 1.441064715385437\n",
      "epoch: 1 step: 1370, loss is 1.6261142492294312\n",
      "epoch: 1 step: 1371, loss is 1.5756011009216309\n",
      "epoch: 1 step: 1372, loss is 1.3279987573623657\n",
      "epoch: 1 step: 1373, loss is 1.6810122728347778\n",
      "epoch: 1 step: 1374, loss is 1.4296385049819946\n",
      "epoch: 1 step: 1375, loss is 1.5289123058319092\n",
      "epoch: 1 step: 1376, loss is 1.366661548614502\n",
      "epoch: 1 step: 1377, loss is 1.572780728340149\n",
      "epoch: 1 step: 1378, loss is 1.8481087684631348\n",
      "epoch: 1 step: 1379, loss is 1.5273722410202026\n",
      "epoch: 1 step: 1380, loss is 1.5481092929840088\n",
      "epoch: 1 step: 1381, loss is 1.652510166168213\n",
      "epoch: 1 step: 1382, loss is 1.627156138420105\n",
      "epoch: 1 step: 1383, loss is 1.395423412322998\n",
      "epoch: 1 step: 1384, loss is 1.3320047855377197\n",
      "epoch: 1 step: 1385, loss is 1.6520872116088867\n",
      "epoch: 1 step: 1386, loss is 1.4844752550125122\n",
      "epoch: 1 step: 1387, loss is 1.519980788230896\n",
      "epoch: 1 step: 1388, loss is 1.4254597425460815\n",
      "epoch: 1 step: 1389, loss is 1.4582099914550781\n",
      "epoch: 1 step: 1390, loss is 1.663176417350769\n",
      "epoch: 1 step: 1391, loss is 1.7413513660430908\n",
      "epoch: 1 step: 1392, loss is 1.4325013160705566\n",
      "epoch: 1 step: 1393, loss is 1.5051031112670898\n",
      "epoch: 1 step: 1394, loss is 1.4081920385360718\n",
      "epoch: 1 step: 1395, loss is 1.5088744163513184\n",
      "epoch: 1 step: 1396, loss is 1.2029695510864258\n",
      "epoch: 1 step: 1397, loss is 1.4583038091659546\n",
      "epoch: 1 step: 1398, loss is 1.4135698080062866\n",
      "epoch: 1 step: 1399, loss is 1.528402328491211\n",
      "epoch: 1 step: 1400, loss is 1.2903286218643188\n",
      "epoch: 1 step: 1401, loss is 1.651487112045288\n",
      "epoch: 1 step: 1402, loss is 1.811093807220459\n",
      "epoch: 1 step: 1403, loss is 1.4519010782241821\n",
      "epoch: 1 step: 1404, loss is 1.3721411228179932\n",
      "epoch: 1 step: 1405, loss is 2.039703130722046\n",
      "epoch: 1 step: 1406, loss is 1.7970322370529175\n",
      "epoch: 1 step: 1407, loss is 1.5576307773590088\n",
      "epoch: 1 step: 1408, loss is 1.3572412729263306\n",
      "epoch: 1 step: 1409, loss is 1.7037913799285889\n",
      "epoch: 1 step: 1410, loss is 1.4643737077713013\n",
      "epoch: 1 step: 1411, loss is 1.6769822835922241\n",
      "epoch: 1 step: 1412, loss is 1.4431480169296265\n",
      "epoch: 1 step: 1413, loss is 1.5346791744232178\n",
      "epoch: 1 step: 1414, loss is 1.3488303422927856\n",
      "epoch: 1 step: 1415, loss is 1.7046409845352173\n",
      "epoch: 1 step: 1416, loss is 1.5970016717910767\n",
      "epoch: 1 step: 1417, loss is 1.6849678754806519\n",
      "epoch: 1 step: 1418, loss is 1.316872000694275\n",
      "epoch: 1 step: 1419, loss is 1.6204274892807007\n",
      "epoch: 1 step: 1420, loss is 1.8027980327606201\n",
      "epoch: 1 step: 1421, loss is 1.6831791400909424\n",
      "epoch: 1 step: 1422, loss is 1.5810935497283936\n",
      "epoch: 1 step: 1423, loss is 1.3473223447799683\n",
      "epoch: 1 step: 1424, loss is 1.4407362937927246\n",
      "epoch: 1 step: 1425, loss is 1.2550406455993652\n",
      "epoch: 1 step: 1426, loss is 1.6675816774368286\n",
      "epoch: 1 step: 1427, loss is 1.6268459558486938\n",
      "epoch: 1 step: 1428, loss is 1.6055495738983154\n",
      "epoch: 1 step: 1429, loss is 1.5068938732147217\n",
      "epoch: 1 step: 1430, loss is 1.4950954914093018\n",
      "epoch: 1 step: 1431, loss is 1.3762717247009277\n",
      "epoch: 1 step: 1432, loss is 1.5317600965499878\n",
      "epoch: 1 step: 1433, loss is 1.2901151180267334\n",
      "epoch: 1 step: 1434, loss is 1.5856587886810303\n",
      "epoch: 1 step: 1435, loss is 1.770923376083374\n",
      "epoch: 1 step: 1436, loss is 1.778681755065918\n",
      "epoch: 1 step: 1437, loss is 1.376274824142456\n",
      "epoch: 1 step: 1438, loss is 1.4942920207977295\n",
      "epoch: 1 step: 1439, loss is 1.4466208219528198\n",
      "epoch: 1 step: 1440, loss is 1.7526510953903198\n",
      "epoch: 1 step: 1441, loss is 1.3881747722625732\n",
      "epoch: 1 step: 1442, loss is 1.5940680503845215\n",
      "epoch: 1 step: 1443, loss is 1.5629340410232544\n",
      "epoch: 1 step: 1444, loss is 1.6335633993148804\n",
      "epoch: 1 step: 1445, loss is 1.4622225761413574\n",
      "epoch: 1 step: 1446, loss is 1.7072337865829468\n",
      "epoch: 1 step: 1447, loss is 1.821104645729065\n",
      "epoch: 1 step: 1448, loss is 1.4320577383041382\n",
      "epoch: 1 step: 1449, loss is 1.5007762908935547\n",
      "epoch: 1 step: 1450, loss is 1.4906930923461914\n",
      "epoch: 1 step: 1451, loss is 1.3522111177444458\n",
      "epoch: 1 step: 1452, loss is 1.488141655921936\n",
      "epoch: 1 step: 1453, loss is 1.3651657104492188\n",
      "epoch: 1 step: 1454, loss is 1.256319522857666\n",
      "epoch: 1 step: 1455, loss is 1.6862505674362183\n",
      "epoch: 1 step: 1456, loss is 1.7665789127349854\n",
      "epoch: 1 step: 1457, loss is 1.6182811260223389\n",
      "epoch: 1 step: 1458, loss is 1.4550668001174927\n",
      "epoch: 1 step: 1459, loss is 1.7551995515823364\n",
      "epoch: 1 step: 1460, loss is 1.6377711296081543\n",
      "epoch: 1 step: 1461, loss is 1.563509464263916\n",
      "epoch: 1 step: 1462, loss is 1.3657541275024414\n",
      "epoch: 1 step: 1463, loss is 1.519540548324585\n",
      "epoch: 1 step: 1464, loss is 1.652221441268921\n",
      "epoch: 1 step: 1465, loss is 1.5691643953323364\n",
      "epoch: 1 step: 1466, loss is 1.4277863502502441\n",
      "epoch: 1 step: 1467, loss is 1.7506890296936035\n",
      "epoch: 1 step: 1468, loss is 1.6741207838058472\n",
      "epoch: 1 step: 1469, loss is 1.272843837738037\n",
      "epoch: 1 step: 1470, loss is 1.736786961555481\n",
      "epoch: 1 step: 1471, loss is 1.2929035425186157\n",
      "epoch: 1 step: 1472, loss is 1.6546177864074707\n",
      "epoch: 1 step: 1473, loss is 1.4148788452148438\n",
      "epoch: 1 step: 1474, loss is 1.5256001949310303\n",
      "epoch: 1 step: 1475, loss is 1.6622529029846191\n",
      "epoch: 1 step: 1476, loss is 1.4899662733078003\n",
      "epoch: 1 step: 1477, loss is 1.5431404113769531\n",
      "epoch: 1 step: 1478, loss is 1.3825268745422363\n",
      "epoch: 1 step: 1479, loss is 1.4408713579177856\n",
      "epoch: 1 step: 1480, loss is 1.7172319889068604\n",
      "epoch: 1 step: 1481, loss is 1.8582959175109863\n",
      "epoch: 1 step: 1482, loss is 1.449710726737976\n",
      "epoch: 1 step: 1483, loss is 1.7834564447402954\n",
      "epoch: 1 step: 1484, loss is 1.5214530229568481\n",
      "epoch: 1 step: 1485, loss is 1.4372026920318604\n",
      "epoch: 1 step: 1486, loss is 1.6744954586029053\n",
      "epoch: 1 step: 1487, loss is 1.239781141281128\n",
      "epoch: 1 step: 1488, loss is 1.8171682357788086\n",
      "epoch: 1 step: 1489, loss is 1.467891812324524\n",
      "epoch: 1 step: 1490, loss is 1.63224196434021\n",
      "epoch: 1 step: 1491, loss is 1.722373366355896\n",
      "epoch: 1 step: 1492, loss is 1.391579508781433\n",
      "epoch: 1 step: 1493, loss is 1.2362070083618164\n",
      "epoch: 1 step: 1494, loss is 1.553748607635498\n",
      "epoch: 1 step: 1495, loss is 1.6653170585632324\n",
      "epoch: 1 step: 1496, loss is 1.648038387298584\n",
      "epoch: 1 step: 1497, loss is 1.4663854837417603\n",
      "epoch: 1 step: 1498, loss is 1.6589401960372925\n",
      "epoch: 1 step: 1499, loss is 1.642501950263977\n",
      "epoch: 1 step: 1500, loss is 1.5271530151367188\n",
      "epoch: 1 step: 1501, loss is 1.2194124460220337\n",
      "epoch: 1 step: 1502, loss is 1.709200143814087\n",
      "epoch: 1 step: 1503, loss is 1.8291339874267578\n",
      "epoch: 1 step: 1504, loss is 1.6275278329849243\n",
      "epoch: 1 step: 1505, loss is 1.701176643371582\n",
      "epoch: 1 step: 1506, loss is 1.511102557182312\n",
      "epoch: 1 step: 1507, loss is 2.0018656253814697\n",
      "epoch: 1 step: 1508, loss is 1.871285080909729\n",
      "epoch: 1 step: 1509, loss is 1.4051779508590698\n",
      "epoch: 1 step: 1510, loss is 1.6158448457717896\n",
      "epoch: 1 step: 1511, loss is 1.5763037204742432\n",
      "epoch: 1 step: 1512, loss is 1.549330711364746\n",
      "epoch: 1 step: 1513, loss is 1.6986325979232788\n",
      "epoch: 1 step: 1514, loss is 1.4937899112701416\n",
      "epoch: 1 step: 1515, loss is 1.3636215925216675\n",
      "epoch: 1 step: 1516, loss is 1.4390138387680054\n",
      "epoch: 1 step: 1517, loss is 1.5765572786331177\n",
      "epoch: 1 step: 1518, loss is 1.3327851295471191\n",
      "epoch: 1 step: 1519, loss is 1.8500159978866577\n",
      "epoch: 1 step: 1520, loss is 1.5130038261413574\n",
      "epoch: 1 step: 1521, loss is 1.5648428201675415\n",
      "epoch: 1 step: 1522, loss is 1.841645359992981\n",
      "epoch: 1 step: 1523, loss is 1.563278079032898\n",
      "epoch: 1 step: 1524, loss is 1.3540558815002441\n",
      "epoch: 1 step: 1525, loss is 1.2999520301818848\n",
      "epoch: 1 step: 1526, loss is 1.479227066040039\n",
      "epoch: 1 step: 1527, loss is 1.439052700996399\n",
      "epoch: 1 step: 1528, loss is 1.523343563079834\n",
      "epoch: 1 step: 1529, loss is 1.7736921310424805\n",
      "epoch: 1 step: 1530, loss is 1.488109827041626\n",
      "epoch: 1 step: 1531, loss is 1.4037431478500366\n",
      "epoch: 1 step: 1532, loss is 1.6090261936187744\n",
      "epoch: 1 step: 1533, loss is 1.372908592224121\n",
      "epoch: 1 step: 1534, loss is 1.3861089944839478\n",
      "epoch: 1 step: 1535, loss is 1.3458943367004395\n",
      "epoch: 1 step: 1536, loss is 1.5197069644927979\n",
      "epoch: 1 step: 1537, loss is 1.7892674207687378\n",
      "epoch: 1 step: 1538, loss is 1.5132163763046265\n",
      "epoch: 1 step: 1539, loss is 1.6609073877334595\n",
      "epoch: 1 step: 1540, loss is 1.6051678657531738\n",
      "epoch: 1 step: 1541, loss is 1.5478509664535522\n",
      "epoch: 1 step: 1542, loss is 1.3263016939163208\n",
      "epoch: 1 step: 1543, loss is 1.4071991443634033\n",
      "epoch: 1 step: 1544, loss is 1.8473620414733887\n",
      "epoch: 1 step: 1545, loss is 1.489766240119934\n",
      "epoch: 1 step: 1546, loss is 1.3866856098175049\n",
      "epoch: 1 step: 1547, loss is 1.609249472618103\n",
      "epoch: 1 step: 1548, loss is 1.5845363140106201\n",
      "epoch: 1 step: 1549, loss is 1.766872525215149\n",
      "epoch: 1 step: 1550, loss is 1.536552906036377\n",
      "epoch: 1 step: 1551, loss is 1.3729199171066284\n",
      "epoch: 1 step: 1552, loss is 1.4209604263305664\n",
      "epoch: 1 step: 1553, loss is 1.8970369100570679\n",
      "epoch: 1 step: 1554, loss is 1.1941660642623901\n",
      "epoch: 1 step: 1555, loss is 1.6467314958572388\n",
      "epoch: 1 step: 1556, loss is 1.4466264247894287\n",
      "epoch: 1 step: 1557, loss is 1.5878314971923828\n",
      "epoch: 1 step: 1558, loss is 1.5458459854125977\n",
      "epoch: 1 step: 1559, loss is 1.4212368726730347\n",
      "epoch: 1 step: 1560, loss is 1.682894229888916\n",
      "epoch: 1 step: 1561, loss is 1.4312125444412231\n",
      "epoch: 1 step: 1562, loss is 1.873915433883667\n",
      "epoch: 1 step: 1563, loss is 1.2877857685089111\n",
      "epoch: 1 step: 1564, loss is 1.5403753519058228\n",
      "epoch: 1 step: 1565, loss is 1.5372519493103027\n",
      "epoch: 1 step: 1566, loss is 1.1948943138122559\n",
      "epoch: 1 step: 1567, loss is 1.5441935062408447\n",
      "epoch: 1 step: 1568, loss is 1.4927031993865967\n",
      "epoch: 1 step: 1569, loss is 1.4664385318756104\n",
      "epoch: 1 step: 1570, loss is 1.737626075744629\n",
      "epoch: 1 step: 1571, loss is 1.7647536993026733\n",
      "epoch: 1 step: 1572, loss is 1.1904475688934326\n",
      "epoch: 1 step: 1573, loss is 1.6519670486450195\n",
      "epoch: 1 step: 1574, loss is 1.7435948848724365\n",
      "epoch: 1 step: 1575, loss is 1.3406410217285156\n",
      "epoch: 1 step: 1576, loss is 1.6141725778579712\n",
      "epoch: 1 step: 1577, loss is 1.6466095447540283\n",
      "epoch: 1 step: 1578, loss is 1.2570585012435913\n",
      "epoch: 1 step: 1579, loss is 1.726068377494812\n",
      "epoch: 1 step: 1580, loss is 1.4545150995254517\n",
      "epoch: 1 step: 1581, loss is 1.3162909746170044\n",
      "epoch: 1 step: 1582, loss is 1.5766322612762451\n",
      "epoch: 1 step: 1583, loss is 1.5823240280151367\n",
      "epoch: 1 step: 1584, loss is 1.3789037466049194\n",
      "epoch: 1 step: 1585, loss is 1.6482974290847778\n",
      "epoch: 1 step: 1586, loss is 1.7630811929702759\n",
      "epoch: 1 step: 1587, loss is 1.3120853900909424\n",
      "epoch: 1 step: 1588, loss is 1.5903061628341675\n",
      "epoch: 1 step: 1589, loss is 1.1423999071121216\n",
      "epoch: 1 step: 1590, loss is 1.7278136014938354\n",
      "epoch: 1 step: 1591, loss is 1.4979451894760132\n",
      "epoch: 1 step: 1592, loss is 1.3710824251174927\n",
      "epoch: 1 step: 1593, loss is 1.4314918518066406\n",
      "epoch: 1 step: 1594, loss is 1.6093580722808838\n",
      "epoch: 1 step: 1595, loss is 1.3583505153656006\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.train(epoch=1, \n",
    "            train_dataset=train_loader, \n",
    "            callbacks=[summary_collector, ckpt_cb, locc_monitor, time_monitor], \n",
    "            dataset_sink_mode=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e349f2f8121bd25fd37a645ebc8a329262b99127061a302d57b19baa7c645ff0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
